<h1 id="música-digital-em-domínios-não-digitais"><a href="#TOC">Música digital em domínios não digitais</a></h1>
<p>[cap:musicaExtra]</p>
<p>As tecnologias expostas neste trabalho tiveram origem em usos reais, advindas de práticas culturais e voltadas para repercussões sociais. Este material humano e artístico é o assunto deste capítulo. Pode-se dividir estes resultados em: experimentos abertos em áudio, música em tempo real, música em tempo diferido, música na matéria e repercussões no tecido social. Estes tópicos são tratados separadamente abaixo em termos do que é mantido <em>online</em>. Estas atividades extrapolam os usos musicais, especialmente devido às finalidades pedagódicas e sociais das práticas audiovisuais. Assim, por completude, alguns materiais didáticos e ferramentas <em>web</em> são descritas. Todos os desenvolvimentos desta dissertação estão em repositórios abertos.<span class="citation"></span></p>
<h2 id="experimentos-abertos-em-áudio-ladspas-wavelets-e-redes-complexas"><a href="#TOC">Experimentos abertos em áudio: LADSPAs, Wavelets e Redes Complexas</a></h2>
<p>O código computacional feito para a arte sonora, incluindo a musical, manifesta-se como cultura pois é fruto de práticas espontâneas, diárias e coletivas. <sup><a href="#fn1" class="footnoteRef" id="fnref1">1</a></sup> A seguir estão alguns exemplos de resultados destas incidências culturais, especificamente em áudio e música.</p>
<h4 id="plugins-ladspa-e-lv2"><a href="#TOC">Plugins LADSPA e lv2</a></h4>
<p>LADSPA (Linux Audio Developers Simple Plugin API) é a API livre de <em>plugins</em> de áudio até a presente data. A última versão é a 1.1, em uso corrente até hoje e é de 2002 segundo os arquivos de cabeçalho dos códigos relacionados. O LADSPA <em>version</em> 2, abreviado lv2, é uma segunda versão do protocolo também estabelecida e estável, mas extensível, motivo pelo qual está sempre em desenvolvimento.</p>
<p>Na síntese, para maior qualidade, pode-se fazer a recomposição a partir do espectro desejado, como na seção [subsec:ruidos]. Menos purista porém mais eficiente é usar uma amostra curta do ruído e reproduzi-la indefinidamente através de <em>cross-fades</em>. Esta eficiência é desejada em um <em>plugin</em>, pois seus usos comuns incluem aplicações em tempo real.</p>
<p>Através deste trabalho, foi disponibilizado um pacote de plugins lv2 que sintetizam os ruídos branco, azul, violeta, rosa e marrom. Os códigos em C e a implementação como plugin lv2 estão no repositório git como pode ser visto em <em>online</em><sup><a href="#fn2" class="footnoteRef" id="fnref2">2</a></sup>.</p>
<p>Já na remoção de ruídos as abordagens são as mais variadas e extremamente dependentes da aplicação. A complexidade dos algorítmos atinge níveis de especialidade em processamento de sinais que já mereceram trabalhos dedicados. Adiante está a implementação de um removedor de ruído <em>’Hum’</em>. Este ruído é geralemte causado pela corrente alternada que alimenta os equipamentos utilizados.</p>
<p>A remoção de ruído <em>Hum</em> é baseada em uma sequência de filtros nó rejeita banda (veja subseção [subsec:filtros]) idealmente dispostos nos harmônicos da frequência de oscilação da corrente elétrica. Cada um destes filtros deve permitir ajustes finos no fator de qualidade e também na frequência central pois trata-se de um sistema real passível dos mais diversos efeitos de distorção do modelo ideal. O código C/C++ e a implementação como <em>plugin</em> está no mesmo repositório disponibilizado para a síntese de ruídos.</p>
<h4 id="wavelets"><a href="#TOC"><em>Wavelets</em></a></h4>
<p>Alguns dos experimentos em <em>wavelets</em> praticados estão em Audioexperiments.<span class="citation"></span> Uma destas investidas superou em muito um experimento e constitui um protocolo de compactação de áudio desenvolvido com o Prof. Rafael Santos Mendes na FEEC/UNICAMP como consequência do mestrado realizado por André Luvizzoto sob orientação do mesmo professor. O método está bem descrito no artigo desenvolvido. Em resumo, o método consiste em particionar o áudio com sobreposições para contemplar aplicações reais sem as distorções de borda da decomposição e recomposição. Em seguida, o áudio é decomposto em árvore <em>Wavelet Packet</em>. Cada folha é então ordenada, os coeficientes com baixa energia eliminados e um polinômio encontrado pelo método dos mínimos quadrados é usado para representar os coeficientes restantes. Detalhes de escrita e leitura dos polinômios e das permutações completam o protocolo. Aplicações para <em>stream</em> de voz em tempo real foram vislumbradas e testes adicionais propostos.<span class="citation"></span></p>
<h4 id="redes-complexas"><a href="#TOC">Redes Complexas</a></h4>
<p>Com o fim de detectar nuanças emocionais no uso da linguagem natural, foram feitos alguns trabalhos em análise de fala e de texto, ambos focados no uso de redes complexas. O primeiro trabalho de processamento de texto foi apresentado na ACL de 2010, em Upsala, na Suécia, pelo Prof. Thiago Pardo, atual presidente da ACL.<span class="citation"></span></p>
<p>Os trabalhos focaram em distinguir polaridade em excertos de textos e de fala. Nos textos, a rede é formada a partir da sequência de palavras, em que cada palavra é um vértice e a sucessão imediata cria uma aresta. O pré-processamento dos textos consiste na lematização e na retiradas das <em>stop-words</em> e da pontuação. Nas falas, as bandas de frequência utilizadas são vértices e a transição entre uma banda e outra é uma aresta. Ambas as redes possuem arestas com peso e podem ser utilizadas como direcionais ou não. Os trabalhos consistiam em formar estas redes com os bancos de dados correlatos, extrair medidas e aplicar reconhecimento de padrão. Ambos os estudos não fazem uso do conteúdo semântico, mas sim topológico dos sinais recebidos, e chegam a mais 80% de cobertura ou precisão em alguns casos, podendo contribuir com métodos tradicionais que fazem uso de redes semânticas, por exemplo.</p>
<h2 id="áudio-e-música"><a href="#TOC">Áudio e música</a></h2>
<p>Este capítulo está dividido em 4 partes: música em tempo diferido (ou seja, que não é realizada em tempo real), música em tempo real, música na matéria (suporte físico em <em>hardware</em>) e música no tecido social (mobilizações humanas). O texto se baseia em exemplos reais da prática musical através do código, com aplicações e em utilização pelo autor, membros do LabMacambira.sf.net, parceiros, colaboradores e por usuários eventuais das naturezas mais diversas.</p>
<h3 id="música-em-tempo-diferido-minimum-fi-e-figgus"><a href="#TOC">Música em tempo diferido: <em>minimum-fi</em> e FIGGUS</a></h3>
<blockquote>
<p>’I also find that intelligent people always respect the intelligence needed to construct a simple structure in a clear way that really works.’</p>
<p><em>Tom Johnson</em></p>
</blockquote>
<p>A realização musical em tempo diferido é o paradigma inicial da música computacional, iniciada com o <em>Music V</em>, depois com o <em>CSound</em>. Pode-se dizer que até hoje é a forma como compositores usualmente pensam a música: concebendo e escrevendo as estruturas que depois são executadas por instrumentistas ou aparelhos eletrônicos. Assim como a composição instrumental permite alguns refinamentos estruturais não presentes na improvisação instrumental, a realização musical em tempo diferido permite um detalhamento maior dos procedimentos do que a realização em tempo real.</p>
<p>O som musical pode ser caracterizado fisicamente, tornando possível a sua síntese digital. Estes sons e a organização deles são objeto do Capítulo 2 da presente dissertação. A utilização destes sons de forma direta, utilizando somente os recursos básicos de justaposição e sobreposição, está na pequena peça <em>minimum-fi</em>. Já o  (FInite Groups in Granular and Unit Synthesis), utiliza os princípios do <em>minimum-fi</em> e utiliza simetrias através de permutações e de Teoria de Grupos, para a composição de músicas. O  gera um EP<sup><a href="#fn3" class="footnoteRef" id="fnref3">3</a></sup> com um único comando. Este é o <em>PPEPPS</em><sup><a href="#fn4" class="footnoteRef" id="fnref4">4</a></sup>, como veremos a seguir.</p>
<p>Desta forma, esta seção exemplifica e explicita - através de dois exemplos reais - o uso de código para a síntese <em>musical</em>, desde as amostras relativas a uma nota com dada frequencia, amplitude e timbre, até a confecção de uma ferramenta derivada, que já incorpora propostas musicais e estruturas mais elaboradas.</p>
<h4 id="minimum-fi"><a href="#TOC"><em>Minimum-fi</em></a></h4>
<p>Existe uma perene matiz estética e também tecnológica dedicada a realizar uma dada tarefa com <em>o mínimo</em> necessário. Na música, esta matiz é igualmente presente e decorre em grande parte do princípio de unidade e coerência<sup><a href="#fn5" class="footnoteRef" id="fnref5">5</a></sup>. Em código computacional, a empreitada para manifestar este princípio ele próprio de forma mínima resultou no <em>minimum-fi.py</em>, código Python em um único arquivo curto que sintetiza a música segundo as estruturas especificadas em linha. Na versão atual, de 2012 mas adiantada em 2011, os algorítmos em Python propriamente ditos somam 53 linhas e incluem apenas 5 funções. Com estas funções, estruturas musicais são criadas padrão a padrão, nota a nota, amostra por amostra. Na prática, e em linguagem cotidiana, as notas formam blocos e estruturas hierarquicamente superiores.</p>
<p>Os princípios, bastante simples, são<sup><a href="#fn6" class="footnoteRef" id="fnref6">6</a></sup>:</p>
<ul>
<li><p>Deve-se ter um mecanismo de síntese sonora que possibilite a geração de unidades sonoras com diferentes timbres, controle sobre a frequência fundamental, duração e volume, como especificado na seção [sec:notaDisc].</p></li>
<li><p>Deve-se ser capaz de construir séries de unidades, sejam sobrepostas (e.g. acordes) ou justapostas (e.g. melodias).</p></li>
</ul>
<p>Para o primeiro item, presta-se o procedimento de busca em tabelas/vetores com formas de ondas em alta resolução, chamado <em>lookup table</em>. O procedimento é barato computacionalmente, com resultados diversificados e tidos como de alta qualidade pois não acrescenta ruídos relevantes ao sinal. Uma descrição do procedimento está na subseção [subsec:lookup].</p>
<p>Através da utilização do <em>lookup</em> sucessivo (procura-se um valor na primeira tabela e este valor indica o valor na segunda tabela a ser utilizado), executa-se um <em>waveshaping</em>. Este procedimento é bastante apreciado pela simplicidade e eficácia na síntese de tímbres diversos e ricos em harmônicos e evolução temporal. Embora uma explicação exaustiva do waveshaping fuja ao escopo deste trabalho, este método se caracteriza pela aplicação de uma função não linear ao sinal de entrada. Neste caso o sinal de entrada é gerado pela primeira busca, a função não linear aplicada é a segunda busca, na outra tabela. Existem muitas formas de se executar um <em>waveshaping</em>, a seguir segue o que usamos no <em>minimum-fi</em>, que se sustenta principalmente por ser leve e simples:</p>
<p>Waveshaping com consultas sucessivas a tabelaspython<sub>s</sub>nippets/lookup<sub>c</sub>ruz.py</p>
<p>O segundo item - dos dois princípios expostos sobre o <em>minimum-fi</em> - presta-se à discretização do espaço musical. Unidades como batidas e notas tornam mais eficiente a comunicação pois a quantidade de estruturas sugeridas é maior e as estruturas são mais explícitas no discreto do que no contínuo. Roederer aponta que as próprias notas dos instrumentos musicais são um reflexo de que é mais eficiente o uso do discreto do que do contínuo para a geração de estruturas musicais.<span class="citation"></span></p>
<p>De fato, unidades bem definidas se mostram úteis na prática musical para fazer sequências de unidades. Quando as unidades são notas com frequência definida, as sequências de unidades justapostas no tempo tendem a ser compreendidas como melodias ou linhas melódicas. As sequências sobrepostas no tempo são comumente compreendidas como acordes, mas podem ser tidas simplesmente como sobreposições circunstanciais de duas ou mais linhas melódicas<sup><a href="#fn7" class="footnoteRef" id="fnref7">7</a></sup>.<span class="citation"></span></p>
<p>As duas construções básicas explicitadas - baseadas na dicotomia melodia/harmonia, horizontalidade/verticalidade, justaposição/sobreposição - são realizadas através das funções <em>fazSequencia</em> e <em>fazAcorde</em> no <em>minimum-fi</em>. Vale notar elas são absolutamente equivalentes em uma análise puramente conceitual, i.e. uma delas pode ser omitida sem perda das possibilidades musicais. Isso fica particularmente óbvio quando se nota que os procedimentos de mixagem e concatenação são plenamente capazes de realizar o que estas funções realizam. Aliás, as funções nada mais são do que usos típicos e quase caricatos destes procedimentos: no <em>fazAcorde</em> a mixagem sobrepõe no tempo todas as unidades, no <em>fazSequencia</em> as unidades são todas juntapostas no tempo.</p>
<p>Como pode-se notar a seguir, as sequências de notas e os acordes, em última instância, são usos específicos das 2 funções de síntese sonora implementadas: <em>lookup</em> e <em>lookupcruz</em>.</p>
<p>Realização de Sequências de Notaspython<sub>s</sub>nippets/fazSequencia.py</p>
<p>Realização de Acordes de Notaspython<sub>s</sub>nippets/fazAcorde.py</p>
<p>A última das cinco funções utilizadas é uma soma amostra a amostra de dois sons. Para isso, é necessário completar com zeros a sequência com o menor número de amostras para somar rapidamente:</p>
<p>Somador (função auxiliar)python<sub>s</sub>nippets/somador.py</p>
<p>Com isso o básico está coberto e o foco vai para a criação de estruturas. Por exemplo: pode-se criar as escalas completamente simétricas na oitava cromática, escalas diatônicas ou até microtonais, como as descritas na subseção [subsec:intervalos].</p>
<p>O estabelecimento de pequenas sequências é bastante útil para reutilizações, variações e geração de materiais derivados, por exemplo: Sequências diversaspython<sub>s</sub>nippets/series.py</p>
<p>O passo seguinte é sintetizar, mixar e concatenar para a obtenção de sequências musicais. A síntese de sequências e acordes pode ser feita desta forma:</p>
<p>Sintetizando sequências e acordespython<sub>s</sub>nippets/seqs<sub>a</sub>cordes.py</p>
<p>Já a síntese de estruturas compostas (e.g. sequências de acordes e sobreposição de linhas melódicas), é feita com os recursos usuais da liguagem: listas em Python e vetores Numpy (mais eficientes em tempo de execução e também na simplicidade do código). Estes procedimentos são praticamente os mesmos se implementados em Scilab, C/C++, Javascript, PHP, etc.</p>
<p>A seguir, para fins didáticos, está a construção de acordes periódicos em python puro<sup><a href="#fn8" class="footnoteRef" id="fnref8">8</a></sup>:</p>
<p>Acordes periódicospython<sub>s</sub>nippets/acordes<sub>p</sub>eriodicos.py</p>
<p>Em posse destas sequências, acordes e recursos da linguagem, são formadas estruturas hierarquicamente superiores através da concatenação de estruturas, da mixagem de estruturas, e da amplificação (ou atenuação) seletiva das mesmas:</p>
<p>Amplificação e mixagempython<sub>s</sub>nippets/amp<sub>m</sub>ix.py</p>
<p>Neste ponto, basta criar músicas e sequências de interesse estético ou para pesquisa. Os encadeamentos dependem de intensões estéticas, entendimentos musicais e estruturas abstratas que mantém a coerência e o interesse em uma peça musical. Para o leitor mais interessado, recomendamos uma visita ao capítulo [cap:resultados] e à <em>toolbox</em> , onde pode-se encontrar o próprio script <em>minimum-fi.py</em> e outros experimentos. A seguir utilizamos esta base apresentada para sintetizar estruturas musicais e então um EP.</p>
<h4 id="figgus-finite-groups-in-granular-and-unit-synthesis"><a href="#TOC">FIGGUS: FInite Groups in Granular and Unit Synthesis</a></h4>
<p>Nesta subseção há um foco nas estruturas musicais. São elas, inseridas em um momento histórico e executadas por instrumentos específicos com as técnicas de época, que constituem em grande parte uma linguagem musical e músicas propriamente ditas. O  constitui uma técnica composicional manifesta em software como ferramenta de síntese de estruturas musicais. Esta técnica consiste na utilização de estruturas matemáticas que representam simetrias para a organização de materiais musicais.</p>
<p>O  foi iniciado em 2006 com o físico-matemático Prof. Adolfo Maia Junior (do IMECC e NICS, ambos da UNICAMP) - bem anterior ao nascimento do <em>minimum-fi</em> - para tratar de simetrias na música com vistas à composição musical através de métodos matemáticos<sup><a href="#fn9" class="footnoteRef" id="fnref9">9</a></sup>. Mais especificamente, a proposta gerou um programa de síntese granular e de estruturas musicais através de Grupos Algébricos. O nome dado foi , sigla de FInite Groups in Granular and Unit Synthesis<sup><a href="#fn10" class="footnoteRef" id="fnref10">10</a></sup>.</p>
<p>Na atual reescrita, embora ainda bastante atrás do  original quanto à interface gráfica, a ferramenta opera diretamente em Python puro, com as biblitecas imbutidas por padrão. Isso permite com que o FIGGUS sintetize todo um EP usando somente os comandos:</p>
<p>Utilizando o FIGGUS para Sintetizar um EPpython<sub>s</sub>nippets/synth<sub>e</sub>p.py</p>
<p>Desta forma, a ferramenta é simples de ser usada para experimentações e implementações adicionais. Outro uso planejado para o é a síntese de timbres e amálgamas sonoros através da Síntese Granular. Embora o foco atual seja outro, cabe algumas breves palavras sobre o assunto.</p>
<p>A Síntese Granular é uma área bem estabelecida tanto na acústica quanto na Computação Musical e se caracteriza pela geração de sons bastante curtos e em quantidade massiva. Tipicamente, os sons possuem entre 5 e 40 milissegundos e a quantidade destes <em>microsons</em><sup><a href="#fn11" class="footnoteRef" id="fnref11">11</a></sup> pode chegar a milhares por segundo. O tratamento específico da síntese granular foge ao escopo deste trabalho. O leitor interessado pode consultar os artigos produzidos sobre Síntese Granular e Teoria de Grupos.<span class="citation"></span> Desta forma, o texto a seguir concentra-se em Grupos Finitos para a síntese de estruturas musicais.</p>
<p>Nas artes é de comum conhecimento o papel absolutamente central que as simetrias possuem. Na música, para citar somente alguns exemplos simples, há os numerosos estudos de simetrias na música de J. S. Bach, os jogos de dados de Mozart e os usos recorrentes da proporção áurea na música de Béla Bártok. Matematicamente, as <em>simetrias</em> são descritas por Grupos, e estes são definidos como um conjunto (seja \(G\)) munido de uma operação (seja \(\bullet\)), formando um grupo \((G,\bullet)\) satisfazendo as propriedades descritas na subseção [estCic]. No , o grão ou unidade sonora é uma classe que possui apenas os seguintes atributos: duração (segundos), frequência (Hz), timbre (identificador para usar mediante implementações convenientes), intensidade (pico \(\in \ [0,1]\)), e duração dos fades (in e out em segundos). O  funciona com base em uma sequência de grãos especificada inicialmente e na qual operam as permutações. Aos grãos em sequência são aplicadas permutações. Para isso é bastante conveniente representar as permutações em classes próprias. A classe do padrão de permutação possui também um período de aplicação da permutação, ou seja, de quantas em quantas leituras da sequência apermutação é aplicada.</p>
<p>Em posse dos grãos, da sequência, das permutações e do padrão de permutações, pode-se realizar a estrutura musical em si. Basta adicionalmente especificar o número desejado de iterações da sequência. Com isso, a sequência de grãos é lida um número de vezes, aplicando as permutações na sequência de grãos segundo o padrão especificado, de forma a resultar em uma sequência musical. Note que se a permutação usar menos elementos que a sequência possui, alguns destes elementos ficarão estáticos nas iterações da sequência no padrão sonoro.</p>
<p>A partir das representações abstratas do padrão musical, são feitos os vetores sonoros da representação digital da música a ser realizada e pode-se escrever um arquivo de áudio propriamente dito. O mais conveniente, neste caso, é escrever um arquivo PCM (Pulse Code Modulation) em algum padrão amplamente utilizado e reconhecido. Ambos WAV e AIFF satisfazem estes requisitos. Mais especificamente, o padrão de CDs é WAV com 44100 amostras por segundo e 16 bits por amostra. As amostras dos vetores sonoros são calculados, por conveniência e convenção, no âmbito \([-1,1]\) e precisam ser normalizados para o âmbito \([-32767,32768]\) e truncados em números inteiros. Depois disso devem ser escritos em um arquivo com os <em>bits</em> como na convenção da linguagem C/C++. A bibioteca <em>struct</em> cuida dessa escrita do inteiro no formato correto, e a biblioteca <em>wave</em> escreve o cabeçalho no formato WAV adequado. Assim, a clase de escrita do vetor sonoro em arquivo comum fica assim:</p>
<p>Escrita do Vetor Sonoro em Arquivo WAVpython<sub>s</sub>nippets/fio.py</p>
<h3 id="música-em-tempo-real-livecoding-e-abeattracker-abt"><a href="#TOC">Música em tempo real: <em>Livecoding</em> e ABeatTracker (ABT)</a></h3>
<p>Tornou-se usual a síntese sonora em tempo real mesmo em <em>laptops</em> populares. Assim, surgiram linguagens de domínio específico, para o áudio e a música, em sua maioria dedicadas - ou ao menos capacitadas - para interação em tempo de execução como <em>Puredata</em>, <em>SuperCollider</em>, <em>ChucK</em> e <em>ixilang</em>. Todos estes são exemplos de linguagens largamente utilizadas para a composição musical e síntese sonora em tempo real. Em outras palavras, estas linguagens possibilitam que o usuário ouça o resultante sonoro do código utilizado e altere o código com resultados imediatos no processamento e sonoridades produzidas.</p>
<p>A exploração estética destas ferramentas em performances musicais é a proposta do <em>Livecoding</em>. Esta prática de <em>performance</em> se dedica especialmente à escrita de código em tempo real com vistas à projeção visual dos códigos enquanto são escritos, junto à projeção sonora que produzem. O ABeatTracker ainda pode ser considerado livecoding, mas já é um intermediário, com traços de um aplicativo ou ferramenta, embora os comandos sejam acessados pela escrita. É uma linguagem com macros para execução de rítmos através de <em>samples</em> em conjunto com instrumentos tradicionais e outras fontes sonoras/musicais externas.</p>
<h4 id="livecoding"><a href="#TOC"><em>Livecoding</em></a></h4>
<p>Recentemente, grupos de ponta em música experimental estão desenvolvendo apresentações musicais públicas baseadas na escrita de código ao vivo. Este é um fenômeno cultural e estético, do qual vale ressaltar os grupos pioneiros <em>Slub</em> e <em>Benoît and the Mandelbrots</em> assim como as ’orquestras de <em>laptops</em>’ <em>PLOrk</em>, <em>SLOrk</em> e <em>DubLOrk</em><sup><a href="#fn12" class="footnoteRef" id="fnref12">12</a></sup>. Usualmente, o código é projetado para que a audiência possa ver o que está sendo escrito, no rítmo em que se escreve, e se projeta também o resultante sonoro por autofalantes.</p>
<p>Estas são motivações presentes em diferentes grupos<sup><a href="#fn13" class="footnoteRef" id="fnref13">13</a></sup>, embora não necessariamente ou da mesmícima forma:</p>
<ul>
<li><p>A apresentação musical, com o uso do computador como instrumento, carece de recursos performáticos visuais dos instrumentos tradicionais. Os gestos são por demais discretos e a concentração do <em>performer</em>/instrumentista é bastante focada na tela do computador. Recursos performáticos são, portanto, preciosíssimos.</p></li>
<li><p>O <em>feedback</em> auditivo do código projetado permite que o espectador infira significados dos códigos. Este recurso do <em>livecoding</em> é usado para desmistificar a programação de computadores e sua aplicação em computação musical, comumente considerada intangível.</p></li>
<li><p>O código em si é um recurso poderosíssimo que permite ao usuário controlar os sons produzidos amostra por amostra ou em escalas maiores de tempo, como notas, compassos, fraseados inteiros ou mesmo em escalas maiores de tempo, como minutos, horas, dias e semanas.</p></li>
<li><p>O compartilhamento do código é usual, leve e eficiente como entrega de tecnologia valiosa para a proposta estética. Isso motiva não só o programador a aplicar seus conhecimentos na música, mas também o músico a adentrar o uso de linguagens de programação para expressão de suas ideias musicais.</p></li>
</ul>
<p>Desta forma, foi iniciada em 2011 uma linha de atuação em <em>livecoding</em> com a criação do duo <em>FooBar</em> composto por Vilson Vieira e o autor deste escrito. Este duo se desenvolveu no trio <em>FooBarBaz</em>/Variáveis Metasintáticas composto pelo duo e por Gilson Beck, este atuante na mesa de som e usando detecção de cor no <em>laptop</em> para incrementar a apresentação. Este trio realizou uma <em>performance</em> no <em>V Festival Contato</em>. É interessante ressaltar que, até onde se sabe, essa foi a primeira apresentação de <em>livecoding</em> no Brasil, e a maior apresentação internacional em tamanho de platéia, estima-se que entre 3 e 5 mil pessoas estavam presentes.</p>
<p>Foi usada a linguagem <em>ChucK</em> por apresentar os recursos que o duo inicial considerou mais apropriados, embora de forma alguma isso seja consensual na prática atual de <em>livecoding</em>. Além disso, a apresentação contou com recursos adicionais para agregar interesse, como a utilização do <em>cowsay</em> para enviar mensagens enquanto se desenrolava a música. Em especial, as trajetórias de um ponto branco no fundo do código sugeria o sono REM como uma experiência coletiva de alteração do estado de consciência. Estes recursos podem ser vistos em uso nos videos demonstrativos<sup><a href="#fn14" class="footnoteRef" id="fnref14">14</a></sup> As linhas do <em>cowsay</em> e o <em>script</em> em linguagem Processing relacionados, assim como maiores detalhes sobre o duo e fotos da apresentação podem ser vistos em <a href="http://wiki.nosdigitais.teia.org.br/FooBarBaz"><code class="url">http://wiki.nosdigitais.teia.org.br/FooBarBaz</code></a>. Estavam presentes no palco também outros membros do labMacambira.sourceforge.net, davam suporte em gravações e outros auxílios: Ricardo Fabbri, Alexandre ’Bzum’, Daniel Penalva e Ivan Marin.</p>
<p>Especificamente sobre a prática do <em>livecoding</em>, duas pessoas executaram <em>scripts</em> em <em>Chuck</em> simultaneamente. A saber, Vilson Vieira executou ritmos, batidas bastante marcadas que serviam como base. Renato Fabbri, autor deste texto, executava linhas fluídas quasi-melódicas que formavam arcos maiores. Havia também interlúdios em que ambos se revesavam com músicas curtas e inusitadas, como em um duelo. A mixagem e espacialização dos dois canais de áudio foram controladas por um <em>patch</em> em Puredata que permitia o <em>cross-fading</em> entre os canais através da detecção do movimento das mãos. Isso foi possível através do objeto em Puredata/GEM chamado <em>color classify</em>, criado por Ricardo Fabbri e posto em uso por Gilson Beck durante a apresentação. Esse mesmo objeto foi utilizado no instrumento AirHackTable, discutido na seção [aht].</p>
<p>A prática de <em>livecoding</em> requer o uso de instruções curtas, que incentivem o improviso musical através do código. Ambos os <em>livecoders</em> usaram bons editores de texto para a escrita dos <em>scripts</em> em <em>ChucK</em>: Vim e Emacs. Renato utilizou aspectos estruturais de alto nivel, arcos longos e estruturas melódicas e minimalistas, como os que seguem:</p>
<p>Interface para controle de parâmetros em tempo realchuck<sub>s</sub>nippets/bar.ck</p>
<p>Ao mesmo tempo, Vilson fez uso de scripts para facilmente especificar a posição do arquivo de áudio em que deve ser iniciada a execução, assim como a velocidade de reprodução do áudio, duração e amplitude. Uma interface simples permitiu a alteração rápida desses parâmetros em tempo de execução.</p>
<p>Interface para controle de parâmetros em tempo realchuck<sub>s</sub>nippets/foo.ck</p>
<p>Classe para <em>sampling</em> de arquivos de áudiochuck<sub>s</sub>nippets/foosp.ck</p>
<p>Estes desenvolvimentos inspiraram a criação de uma linguagem específica para <em>livecoding</em>, chamada <em>Vivace</em>, que está em constante desenvolvimento e tem seu código e maiores detalhes disponíveis online de forma livre<sup><a href="#fn15" class="footnoteRef" id="fnref15">15</a></sup>. Já foram realizadas novas apresentações utilizando essa linguagem, a notar: Hacklab do Velho, em São Carlos (SP), Semana da Comunicação FAAP (SP), Palco UFSCar (SP) e Semana Nacional da Ciência e Tecnologia (SP). Nelas houve a predominância do apelo às mídias populares, em uma reinauguração do gênero da <em>pop art</em>: trechos de vídeos de uma novela brasileira: ’Avenida Brasil’ eram rearranjados em tempo real em sincronia com linhas rítmicas e melódicas<sup><a href="#fn16" class="footnoteRef" id="fnref16">16</a></sup>.</p>
<p>Catalizadas pela aproximação de &quot;Gera&quot; Magela, Caleb Mascarenhas Luporini e Guilherme Lunhani as apresentações e amadurecimeentos de 2012 desembocaram na escrita coletiva de códigos (acesso e controle da platéia no código que está sendo escrito), utilização de monstros e <em>ascii art</em> e e sonoridades bizarras. Com isso, está sendo observado e proposto um gênero de <em>livecoding</em> batizado de ’<em>Freakcoding</em>’, possivelmente o primeiro subgênero de <em>livecoding</em>. Foi escrito um <em>manifesto</em> sobre o <em>freakcoding</em> e um artigo acadêmico. Estão sendo planejados mais apresentações e desenvolvimentos tecnológicos, todos motivados pelo <em>livecoding</em> e este subgênero brasileiro que está despontando.</p>
<h4 id="abeattracker-abt"><a href="#TOC">ABeatTracker (ABT)</a></h4>
<p>O ABT é uma linguagem que dispara linhas rítmicas através de macros. Nestas, especifica-se as células rítmicas, amostras sonoras que são utilizadas como conteúdo sonoro destas linhas, modos de leitura destas amostras sonoras e variáveis randômicas utilizadas para execução da linha. Além disso, o ABT dispõe de variáveis globais que podem ser alteradas a qualquer momento pelo usuário também com macros, como BPM, volume, velocidade de leitura das amostras e variáveis randômicas globais (que se somam às individuais). As macros pré-estabelecidas constituem um conjunto de recursos pré-estabelecidos bem definido, o que contrasta com a ideia de uma ’linguagem de programação’ que tenha capacidades mais amplas. De qualquer forma, linguagens com domínios específicos não são raras e por vezes o ABT foi descrito como uma linguagem por usuários e interessados.</p>
<p>O ABT é um instrumento computacional essencialmente rítmico. Junto a esta proposta, está a necessidade da utilização em conjunto com outros instrumentos, externos ao ABT, ao computador em que o ABT está sendo executado e possivelmente externo com relação a qualquer computador. Para isso foi elaborado o ABD (ABeatDetector), no qual o usuário tamborila no teclado do computador os rítmos que estiver ouvindo ou imaginando para que o ABT sincronize o pulso e utilize células rítmicas relacionadas. A análise feita pelo ABD resulta em uma série de rítmos explicitados por sequências de compassos que encapsulem durações regulares do rítmo tamborilado. Estes rítmos relacionados ao tamborilar do usuário são chamados de harmônicos e podem ser selecionados prontamente para o disparo de linhas melódicas no ABT. De fato, atualmente o ABD é parte do ABT. Todo o código do ABT está disponivel online junto à documentação para uso<sup><a href="#fn17" class="footnoteRef" id="fnref17">17</a></sup>.</p>
<h3 id="música-na-matéria-ekp-e-aht"><a href="#TOC">Música na matéria: EKP e AHT</a></h3>
<p>[aht]</p>
<p>Embora o foco deste trabalho seja a exploração musical através de códigos computacionais, nesta seção está uma explicação simples, clara e factual de como estas investidas transcendem o código e até mesmo a música em si. Por vezes, mais relevantes que os próprios desenvolvimentos são as mobilizações criadas nos entornos e os engajamentos. A seguir estão dois trabalhos que geraram alguma mobilização, resultando em trocas, reuniões, desenvolvimentos, pesquisas e apresentações propriamente ditas. O primeiro utiliza o estado do hardware como entrada, o segundo consiste na flutuação de origamis para a obtenção de um instrumento musical lúdico.</p>
<h4 id="emotional-kernel-panic-ekp"><a href="#TOC">Emotional Kernel Panic (EKP)</a></h4>
<p>Em 2008, em colaboração intensa com o CDTL (Centro de Desenvolvimento de Tecnologias Livres)<sup><a href="#fn18" class="footnoteRef" id="fnref18">18</a></sup> foi lançada a ideia de utilizar o estado do sistema operacional - especialmente o kernel linux - para gerar de sons. Surge então o ’<em>Emotional Kernel Panic</em>’ (EKP) na parceria de Felipe Machado (então coordenador do CDTL), Ricardo Brazileiro (artista conhecedor de tecnologias livres) e o primeiro autor do presente trabalho.</p>
<p>Foram reconhecidas três finalidades para esta exploração do sistema operacional:</p>
<ul>
<li><p>Pedagógica, através da utilização de um sentido que não o visual para a tarefa de analisar o sistema.</p></li>
<li><p>Artística, para apresentações musicais/audiovisuais.</p></li>
<li><p>Monitoramento do sistema operacional, pela emissão de sons periódicos relativos à carga de processamento, memória, uso de rede, etc.</p></li>
</ul>
<p>Esta empreitada se desdobrou em apresentações na Conferência Internacional de PureData (2009), SESC (2009), e no III Festival Contato (2009). No Festival Internacional de Software Livre de 2010 foi apresentada uma pesquisa sobre o EKP com diversos patches feitos por Ricardo Brazileiro e Felipe Machado, assim como amadurecimentos da proposta. Uma implementação conceitual do EKP está aqui: <a href="http://trac.assembla.com/audioexperiments/browser/ekp-base"><code class="url">http://trac.assembla.com/audioexperiments/browser/ekp-base</code></a>.</p>
<h4 id="airhacktable"><a href="#TOC">AirHackTable</a></h4>
<p>A AirHackTable (AHT) é um instrumento musical eletrônico controlado por origamis (dobraduras de papel), construída na forma de uma mesa. Nela, uma rede de <em>coolers</em> reciclados faz flutuar origamis de geometria e cores variadas. Os movimentos dos origamis são captados por <em>webcam</em> e interpretados em tempo real por software de processamento de imagens, gerando padrões que controlam a transformação sonora da música. Desta forma, pode-se dizer que os sons gerados refletem o vôo dos origamis de acordo com suas geometrias (que geram trajetórias de vôo caracteristicas). Dito de maneira mais teórica, as estruturas fora do tempo (geometrias dos origamis) estão sendo mapeadas em estruturas temporais (trajetórias dos origamis em seus vôos), o que é um macete da música erudita desde tempos antigos.</p>
<p>Na versão atual da AirHacktable, cada cor (vermelho, amarelo, azul, verde, preto, ou branco) controla uma voz. Já a posição do origami na mesa controla aspectos do som. Como um exemplo, pode-se configurar a AHT de forma que, se o origami está flutuando para a esquerda, o som também se move para esquerda, se o origami está mais próximo da câmera, o volume é maior, e se está mais afastando do operador, o som se torna mais agudo.</p>
<p>A AHT segue a filosofia de desenvolvimento contínuo, comum ao <em>software</em> livre, que atualmente foi incorporado ao movimento chamado <em>hardware</em> livre ou aberto. A concepção inicial foi de Chico Simões (mestre de maracatu e conhecedor de tecnologias livres) e o autor deste texto, através do amadurecimento de possiveis instrumentos musicais de papel. Logo neste estágio inicial, foram aproximados Vilson Vieira, Ricardo Fabbri, Fábio Simões e Daniel Penalva em reuniões presenciais. Destas reuniões saíram ideias diversas, escritas em um <em>pad</em> aberto<sup><a href="#fn19" class="footnoteRef" id="fnref19">19</a></sup>, e, por fim, a AHT em si.</p>
<p>A primeira realização da mesa se deu no V Festival Contato, no <em>Espaço Macambira</em>, com apresentação na Teia Casa de Criação, durante o mesmo festival. A estrutura da mesa é também de papel (papelão), e foi feita por Francisco Simões por pesquisa própria. O uso do reticulado de ventoinhas foi fruto de discussões coletivas e de experimentos do coletivo labMacambira.sourceforge.net, com especial empenho do Francisco e uso de sua luthieria de percussão: <em>Tora Tambores</em>.</p>
<p>Em 2012 houve a aproximação de Caleb Mascarenhas Luporini e &quot;Gera&quot; Magela nesta frente de atuação. Foram feitas oficinas no SESC Pinheiros e SESC Belenzinho e apresentação no AVAV<sup><a href="#fn20" class="footnoteRef" id="fnref20">20</a></sup>, todas com o foco na construção e uso da AHT.</p>
<h3 id="música-no-tecido-social-sabrina-kawahara-audioexperiments-estudiolivre.org-cdtl-juntadados.org-devolts.org-msst-labmacambira.sf.net"><a href="#TOC">Música no tecido social: Sabrina Kawahara, Audioexperiments, EstudioLivre.org, CDTL, juntaDados.org, Devolts.org, MSST, LabMacambira.sf.net</a></h3>
<p>A singularidade do cruzamento artístico, cultural e tecnológico dos trabalhos expostos também é desenvolvida por diferentes grupos. Os propósitos de compartilhamento e apropriação tecnológica estão no cerne destas comunidades, de forma que as investidas naturalmente tomam rumos engajados socialmente. Em especial, o empoderamento civil e a criação de um patrimônio tecnológico da humanidade são consequências imediatas das posturas de compartilhamento e apropriação citados. Segue uma lista parcial dos grupos mais relevantes para o presente trabalho, tanto pela influência que estes grupos/redes tiveram na formação e atuação do primeiro autor quanto pela ressonância que os trabalhos aqui expostos encontram.</p>
<ul>
<li><p>Sabrina Kawahara: formado pelo autor deste texto, Guilherme Lunhani, Guilherme Rebecchi, Israel Laurindo, Rodrigo Felício e Otávio Martigli, este grupo dedicou-se à realização de apresentações públicas com peças musicais, bem como à escrita de peças e textos através de dinâmicas coletivas.</p></li>
<li><p>Audioexperiments (Æ): iniciado por Tiago Tavares, o autor do texto e aproximações de Daniel Pastore. O grupo se dedicou à publicação de textos didáticos e produção de código em repositórios públicos. Em destaque, o repositório no <em>Assembla</em> possui códigos em Python e C/C++ que realizam diversos experimentos em áudio, música e inteligência artificial<sup><a href="#fn21" class="footnoteRef" id="fnref21">21</a></sup>.</p></li>
<li><p>Estudios Livres: são ambientes de estudo, produção e distribuição de mídias livres. Formam, em conjunto, um movimento e uma rede social brasileira, tida como única no mundo, embora o conceito seja conhecido internacionalmente<sup><a href="#fn22" class="footnoteRef" id="fnref22">22</a></sup>. Tanto a comunidade quanto a lista operacional e plataforma da comunidade foram essenciais nos amadurecimentos de questões relacionadas às mídias livres e ao uso de Python para áudio e música, como pode ser visto na Carta Mídias Livres e no Tutorial de Python para Áudio e Música, abordados abaixo em [subsec:tuts] deste mesmo Apêndice.</p></li>
<li><p>CDTL (Centro de Desenvolvimento de Tecnologias Livres): foi uma associação civil com base em Recife/PE, dedicada ao desenvolvimento e difusão de tecnologias livres. Esta associação proporcionou suporte para os primeiros materiais didáticos e investidas em plugins de áudio através do convênio estabelecido com o Ministério da Cultura de Pontão de Cultura Digital.</p></li>
<li><p>JuntaDados.org: é um grupo com membros em todo o território brasileiro e ainda ativo. Embora tenha havido um momento de maior institucionalidade, durante o convênio de Pontão de Cultura Digital com o Ministério da Cultura e parceria com a Universidade Estadual da Bahia (UNEB), o grupo é marcado pela descentralização e autonomia dos envolvidos. O autor é membro fundador e ativo do juntaDados, em conjunto, especialmente, com Fabiana &quot;Goa&quot; Sherine e Marcelo Soares Souza. Neste grupo foram desenvolvidas ideias como o EKP, o ABT, os tutoriais de Python para Áudio e escritos socialmente engajados.</p></li>
<li><p>Devolts.org: responsável por Esporos de Pesquisa e Experimentação do programa Cultura Viva, os &quot;Jardins Devolts&quot; cultivam listas nacionais de trocas de conhecimentos livres e cultivo de informações voltadas para música com usos especiais de <em>PureData</em>, <em>Arduinos</em> e <em>Python</em>. Este é um meio de troca rico e alternativo a alguns meios mais politizados, como a lista do <em>estudiolivre.org</em>.</p></li>
<li><p>MSST (Movimento dos Sem Satélite): com um manifesto e propostas coerentes, mas nada óbvias, como a demanda específica por satélites civis, abordagens tecnomágicas, o grupo é uma instância iniciática em termos tecnológicos e conceituais.</p></li>
<li><p>LabMacambira.sf.net: espaço virtual de programadores dedicados ao audiovisual e propósitos socialmente engajados. Fundado em parceria do autor deste trabalho e de Vilson Vieira, Daniel Marostegan e Carneiro e Ricardo Fabbri e de parte dos investimentos da segunda etapa do convênio de Pontão de Cultura Digital Nós Digitais, da Teia Casa de Criação. Em um primeiro momento, membros foram remunerados como mentores e como aprendizes<sup><a href="#fn23" class="footnoteRef" id="fnref23">23</a></sup>. Em um segundo momento, aproximações e parcerias diversas ocorreram. Detalhes estão mas subseções seguintes de <em>Web</em> e materiais didáticos.</p></li>
<li><p>Submidialogia: grupo bastante relacionado aos encontros de ’<em>submidialogias</em>, ao Descentro e às ideias de submídia. A lista de emails do grupo possui membros de diversos dos outros grupos citados.</p></li>
<li><p>Outros grupos relacionados: o MuSa foi um grupo de Joinville importante neste trabalho por ter sido fundado por Vilson Vieira, que possui presença forte no LabMacambira.sf.net e nos trabalhos aqui descritos. A Metareciclagem é uma rede engajada em apropriação tecnológica e empoderamento civil com bastante presença dos outros grupos aqui citados e muito movimentada. A lista de emails ligada ao movimento Transparência Hacker foi também de importância central como espaço focado em questões de transparência e dados abertos. A Casa de Cultura Tainã é uma entidade cultural de Campinas/SP, referência no uso de tecnologias livres e atividades socialmente engajadas, e onde muito do que está aqui descrito foi inspirado. O Hacklab do Velho foi fundado por pessoas ligadas ao labMacambira.sf.net e por moradores do alojamento velho da USP, campus de São Carlos, e acolheu diversas experimentações e articulações. Outros grupos com alguma importância foram a Casa Fora do Eixo de São Carlos, a Teia Casa de Criação, a Nuvem estação rural de experimentação tecnológica e o grupo que organiza o AVAV Áudiovisual Ao Vivo, ligado ao Epicentro Cultural.</p></li>
</ul>
<h2 id="materiais-didáticos"><a href="#TOC">Materiais didáticos</a></h2>
<h3 id="tutoriais-em-texto-e-código-python-filtros-e-nyquist-plugins-lv2-metrics-carta-mídias-livres-contra-cultura-digital"><a href="#TOC"> Tutoriais em texto e código: python, filtros e nyquist, plugins lv2, metrics, carta mídias livres, contra-cultura digital </a></h3>
<p>[subsec:tuts]</p>
<ul>
<li><p><strong>Tutorial de python para áudio e som</strong></p>
<p>Este tutorial foi apresentado parcialmente em Berlim no LAC 2007 e, desde então, foi melhorado algumas vezes. Esta primeira versão ficou resumida em forma de texto na plataforma Estúdio Livre<sup><a href="#fn24" class="footnoteRef" id="fnref24">24</a></sup>. Um agradecimento especial para Fábio Furlanete e Marília Chiozo pelas contribuições. Em 2010 a Associacao Python Brasil escolheu este trabalho, então já mais maduro, para ser apresentado no Festival Internacional de <em>Software</em> Livre, em Porto Alegre. Como consequência, foi feita uma série de video-tutoriais<sup><a href="#fn25" class="footnoteRef" id="fnref25">25</a></sup>. Os vídeos são tidos como iniciáticos em Python por diferentes pessoas ligadas ao movimento de <em>software</em> livre.</p></li>
<li><p><strong>Tutoriais de filtros e amostragem via python </strong></p>
<p>Voltados para explicitar fundamentos de áudio digital, estes tutoriais são baseados em pequenos <em>scripts</em> escritos em Python que exploram conceitos puntuais. Pequenas explicações são dadas com o intuito de orientar a exploração inteligente destes <em>snippets</em>. <em>Teorema de Amostragem</em>: estes <em>scripts</em> executam experimentações ilustrativas com o Teorema de Nyquist e figuras. <em>Filtros</em>: além de filtros FIR e IIR simples, duas utilizações clássicas destes filtros estão implementadas de forma didática: Wavelets (FIR) e Quad (IIR). Estes códigos didáticos podem ser baixados do repositório <em>audioArt</em><sup><a href="#fn26" class="footnoteRef" id="fnref26">26</a></sup>.</p></li>
<li><p><strong>Tutorial de plugins lv2</strong></p>
<p>Dadas as dificuldades que o desenvolvimento dos <em>plugins</em> de áudio apresenta, foi feito um tutorial passo a passo com <em>plugins</em> operantes em todas as etapas. Os códigos e os textos estão todos em repositório<sup><a href="#fn27" class="footnoteRef" id="fnref27">27</a></sup>.</p></li>
<li><p><strong>Microtutoriais Django<sup><a href="#fn28" class="footnoteRef" id="fnref28">28</a></sup> </strong></p>
<p>Estes ’microtutoriais’ são baseados nos conceitos de <em>scripts mínimos</em> e <em>alterações puntuais</em>. O primeiro conjunto de microtutoriais é dedicado a reconstruir o tutorial oficial do django de forma condensada e não prolixa. O segundo destes conjuntos é dedicado a instrumentalizar de fato o leitor com o entendimento do funcionamento dos princípios fundamentais deste <em>framework</em>. A importância deste material reside no fato de ser uma das primeiras incidências dos <em>scripts</em> mínimos, muito usados no labMacambira.sourceforge.net para passagens de tecnologias de forma precisa.</p></li>
<li><p><strong>Figusdevpack (FDP)</strong></p>
<p>Idealizado como um meio de interação da comunidade de Python e Música para compartilhamento de <em>scripts</em> inteiros e excertos, o FDP foi Baseado principalmente em documentação organizada sobre as práticas e as bibliotecas existentes para Python. Parte desta documentação é proposta como <em>scripts</em> que fazem uso de objetos e módulos de forma isolada, puntual. Este trabalho foi aceito na maior conferência de áudio em linux, a Linux Audio Conference de 2008 (LAC2008) e foi reativado algumas vezes pelo autor deste texto em conjunto com Vilson Vieira, Ivan Marin e outros desenvolvedores. Este projeto está documentado na plataforma do Estúdio Livre<sup><a href="#fn29" class="footnoteRef" id="fnref29">29</a></sup>.</p></li>
<li><p><strong>Carta mídias livres</strong></p>
<p>Texto criado em decorrência da participação do autor do presente escrito na comissão de seleção no “Prêmio Mídias Livres”, a convite do Ministério da Cultura por “notório saber”. A participação consistiu em avaliar os inscritos no Prêmio Mídias Livres e distribuir 4 milhões de reais dentre categorias regionais e nacionais. Esta carta é um documento único, deixando às claras o conceito de Mídias Livres como não aprisionadas pelo conceito de propriedade, ou seja, que priorizam a sua livre circulação e a possibilidade de geração de materiais derivados. Há o viés de priorizar processos colaborativos, comunitários e setores da sociedade menos contemplados na geração e circulação midiática<sup><a href="#fn30" class="footnoteRef" id="fnref30">30</a></sup>.</p></li>
<li><p><strong>Philosometrics</strong></p>
<p>Em decorrência deste trabalho, surgiu o Musimetrics, o Cinemetrics e o Literametrics. Uma publicação no <em>Journal of Statistical Mechanics: Theory and Experiment</em> condensa e compara as análises de filosofia e música.<span class="citation"></span> Além disso, é uma utilização das ciências duras para assunto mais incidente em ciências humanas.</p></li>
<li><p><strong>Textos socialmente engajados </strong></p>
<p>O uso de pseudônimos é um costume apreciado em diversos meios. As pesquisas informais confirmam vantagens desta prática<sup><a href="#fn31" class="footnoteRef" id="fnref31">31</a></sup>. Em especial utiliza-se pseudônimos para auxiliar a despersonificação, gerando textos menos presos à satisfação da auto-imagem. Como resultado, além de aumento de produtividade, constuma-se conseguir também compreensões diferentes. Destes textos, dois foram publicados em duas publicações relevantes: O Contracultura Digital e o Peixe Morto, este último relacionado aos submidialogias<sup><a href="#fn32" class="footnoteRef" id="fnref32">32</a></sup>.</p></li>
</ul>
<h3 id="screencasts-e-outros-materiais-em-video"><a href="#TOC"><em>Screencasts</em> e outros materiais em video</a></h3>
<ul>
<li><p><strong>Python para áudio e música:</strong></p>
<p>Relacionado ao tutorial em texto citado acima, foram feitos videos sobre a utilização da linguagem Python, em grande parte através de usos em <em>IPython</em>, para leitura e escrita de arquivos de áudio com bibliotecas padrão e externas, assim como uso básico de wavelets,OSC e outros recursos.</p></li>
<li><p><strong>Canal Macambira no Vimeo</strong></p>
<p>Atualmente, são mais de 700 videos sobre sessões de <em>hacking</em> e experiências de arte e tecnologia. Feitos pelo LabMacambira.sourceforge.net, o canal tem videos do autor deste escrito, e de: Larissa Arruda, Alexandre Negrão, Lucas Zambianchi, Andres Martano, Fernando Gorodscki, Chico Simões, Daniel Marcicano, Daniel Pizetta, Daniel Penalva, Nivaldo Bondança, Danilo Shiga, Marcos Murad, Ricardo Fabbri e Vilson Vieira<sup><a href="#fn33" class="footnoteRef" id="fnref33">33</a></sup>. Exemplos especiais dos videos disponibilizados no canal são:</p>
<ul>
<li><p><em>Livecoding</em>: foram expostos os processos utilizados na apresentação de <em>livecoding</em> com ChucK, vacas e sono REM<sup><a href="#fn34" class="footnoteRef" id="fnref34">34</a></sup>.</p></li>
<li><p>Raspagem de dados: menos artístico e mais engajado, é uma breve explicação sobre o processo de raspagem de dados em páginas HTML<sup><a href="#fn35" class="footnoteRef" id="fnref35">35</a></sup>.</p></li>
<li><p>Outros videos incluem gambiarras artísticas, uso de scripts/ferramentas para audiovisual, etc.</p></li>
</ul></li>
</ul>
<h2 id="web"><a href="#TOC">Web</a></h2>
<p>Esta dissertação é focada em uma descrição física do áudio digital enquanto música e usos em código desta abordagem. Como pode-se observar acima, neste mesmo Apêndice, este trabalho, disponibilizado e concebido como tecnologias livres, tem dimensões bastante consideráveis em aspectos socialmente engajados e artísticos. Esta seção exibe os feitos que extrapolam a música por completo.</p>
<h3 id="tecnologias-sociais-sítios-conteúdos-e-articulação"><a href="#TOC">Tecnologias sociais: Sítios, Conteúdos e Articulação</a></h3>
<h4 id="sítios"><a href="#TOC">Sítios</a></h4>
<p>Alguns <em>sites</em> foram criados, como parte de trabalhos engajados e tentativa de manutenção das estruturas do labMacambira.sourceforge.net. O primeiro site com alguma relevância seja talvez o site para a Casa dos Meninos / LIDAS, em que foi feita uma estrutura para acolher as conferências setoriais e municipais dos direitos da criança e do adolescente da cidade de São Paulo. Outros exemplos, mais recentes, são as páginas para a Rede Nacional de Cultura Ambiental Afrobrasileira e os blogs para a Rio+20 das secretarias do Ministério da Cultura.</p>
<p>Nest meio tempo também foram criadas várias ferramentas <em>web</em>, que rodam em navegadores comuns, como o <em>Firefox</em>. Destes, vale citar<sup><a href="#fn36" class="footnoteRef" id="fnref36">36</a></sup>:</p>
<ul>
<li><p>SOS: sistema para coleta e difusão de conhecimentos populares e étnicos ligados à saúde.</p></li>
<li><p>Ágora Communs, Novo Ágora: sistema de deliberações <em>online</em>.</p></li>
<li><p>Economia Criativa: rascunho de sistema para facilitar circulação de bens me 4 eixos: venda de produtos, aluguél de equipamentos, aluguél de lugares e prestação de serviços.</p></li>
<li><p>Catálogo de Ideias: ferramenta simples para catálogo de ideias sobre temas diferentes, junto a palestras, etc.</p></li>
<li><p>Mapeamento dos 301 pontos de SP: mapeamento dos 301 pontos de cultura estaduais.</p></li>
<li><p>Mapas Coletivos: ferramenta para criação e publicação de mapas com possibilidades colaborativas. Bastante usada por pessoas de meios diferentes pela usabilidade e proposta pertinente.</p></li>
<li><p>Leitura de <em>shapefiles</em> no Mapas de Vista: implementação de leitura de arquivos do tipo <em>shapefile</em> com dados geográficos em um plugin e tema de <em>Wordpress</em>.</p></li>
<li><p>Maper: atualmente, um rascunho de uma ferramenta de publicação de Mapas com Vistas a publicações no facebook e outros formatos.</p></li>
</ul>
<h4 id="conteúdos"><a href="#TOC">Conteúdos</a></h4>
<p>Os conteúdos estão em diversas páginas da <em>wiki</em> do Nós Digitais e do Estúdio Livre.<sup><a href="#fn37" class="footnoteRef" id="fnref37">37</a></sup> Tratam principamente de documentações de <em>software</em> e receitas para acesso a repositórios ou instalações. Nestas documentações também estão apontamentos de recursos artísticos e apresentações realizadas com a AHT, <em>livecoding</em> e outras propostas. Chama a atenção a quantidade de instâncias de <em>etherpads</em> usadas, o labMacambira.sf.net chega a criar vários destes epads por dia, para escrita simultânea a várias mãos<sup><a href="#fn38" class="footnoteRef" id="fnref38">38</a></sup>.</p>
<h4 id="articulação-disponibilização-e-desenvolvimento-conjunto"><a href="#TOC">Articulação, disponibilização e desenvolvimento conjunto</a></h4>
<p>As articulações se dão principalmente através do IRC, do AA e de emails, além de conversas presenciais. O IRC é uma forma de comunicação leve e focada na comunicação coletiva. O AA está explicado abaixo. Não menos importante são os <em>Etherpads</em> e as páginas <em>wiki</em> criadas, permitindo proposições e escritas rápidas por diferentes pessoas.</p>
<h4 id="aa"><a href="#TOC">AA</a></h4>
<p>Uma das tecnologias desenvolvidas nesta empreitada se propõe a estabelecer formas de compartilhar andamentos e processos. As motivações incluem transparência civil e organização assíncrona de grupos descentralizados. O sistema se baseia no envio de mensagens periódicas sobre as atividades que estão sendo desempenhadas, evitando foco em produtos e propaganda. A ferramenta pode ser usada no IRC ou em linha de comando. Existem algumas interfaces <em>web</em> para exibir as mensagens e dados do AA<sup><a href="#fn39" class="footnoteRef" id="fnref39">39</a></sup>. Estão planejados desenvolvimentos do AA para que seja utilizável através do <em>chat</em> em diferentes redes sociais. Também está sendo considerada a utilização do AA para distribuição de verba de acordo com as dedicações comprovadas pelos mecanismos do AA, tornando possível que se trabalhe de forma remunerada pelo bem comum.</p>
<h2 id="momento-atual-e-previsões"><a href="#TOC">Momento atual e previsões</a></h2>
<p>Está sendo escrito sobre <em>livecoding</em>, especialmente ligado ao Vivace e ao subgênero <em>Freakcoding</em>, que recebeu um manifesto recentemente. Um artigo explicando os vínculos entre a proposta estética e o programa Vivace está sendo escrito por várias mãos. O georeferenciamento, perene na atuação do labMacambira.sf.net, tomou agora a direção de arrumar animações de fluxos em arestas de grafos dispostos em mapas. Apresentações artísticas estão previstas para uso da AHT e experimentações de <em>livecoding</em>. Um <em>crowdfunding</em> foi feito para experimentação de difusão na rede e respostas, com estudos em andamento sobre comportamentos no <em>Facebook</em> e listas de <em>emails</em>. O Cultura Viva, programa do governo federal que apoiou parte destas iniciativas, indica fortalecimento, tornando-se política de estado, não somente do governo que a implantou. As aproximações federais são observadas também por parte do MEC (Ministério da Educação), com a prestação de uma acessoria em 2012, e do MinC, devido à Lei Cultura Viva. Comunicações mais moderadas continuam com o MCTi, MC e MMA. Os canais de IRC e emails do labMacambira.sf.net estão bastante ativos e focados em execução de tarefas, e, dentre os desenvolvimentos em código previstos, está o aproveitamento desta dissertação para implementações em JavaScript, experimentações, apresentações, publicações, usos didáticos e estudo propriamente dito. O alto rendimento do labmacambira.sourceforge.net no GSoC de 2012, e o forte empenho de alunos do IPRJ/UERJ, de Nova Friburgo, através do suporte oferecido pelo prof. Ricardo Fabbri, da mesma instituição, apontam para uma vida própria que estes andamentos tomaram. Em São Paulo, a atuação de Geraldo Magela e Caleb Luporini se propõe a difundir estas práticas em circuitos artísticos e de ensino. Por emails privados, listas ou IRC, os participantes do grupo estabelecem trocas com pessoas em diversas localidades no Brasil e no mundo para fins de desenvolvimento tecnológico, trabalhos, elaborações didáticas e convivências, mesmo que brandas, por protocolos de comunicação. A proposta no momento é focar em algo que contemple estas coisas todas da melhor forma, aliando cobertura dos interesses e precisão no aprofundamento, o que está despontado nos estudos de <em>logs</em> de IRC e listas de <em>emails</em>, com bancos de dados massivos e públicos. As possibilidades artísticas, engajadas, científicas e de instrução e interesse pessoal são confluentes através da modelagem por redes complexas, o que sugere a unificação do conhecimento, traço marcante da área e observado com frequência.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>As chamadas culturas biopunk, ciberpunk, cipherpunk, hacker, digital e outras mais, possibilitadas pelos desenvolvimentos em telecomunicação, dizem respeito em menor ou maior grau à produção de código computacional como uma manifestação cultural. As elaborações de conceitos e ferramentas voltados para o compartilhamento nestas culturas estão na gênese do que se entende como ’Cultura Livre’.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Este link é o <em>README</em> do repositódio <em>git</em> com os códigos dos plugins: <a href="http://labmacambira.git.sourceforge.net/git/gitweb.cgi?p=labmacambira/LV2;a=summary"><code class="url">http://labmacambira.git.sourceforge.net/git/gitweb.cgi?p=labmacambira/LV2;a=summary</code></a>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p><em>Extended Play</em>, um álbum musical maior que um single mas menor que um LP (<em>Long Play</em>) inteiro.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Pure Python EP: Projeto Solvente.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Este princípio tanto é fundamental que as escolas musicais possuem técnicas específicas, músicas possuem suas próprias convenções mantidas por toda a sua duração, os arcos mantém características, enfim, podemos até mesmo concluir que as simetrias definem o escopo musical.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>Osvaldo Lacerda, em seu livro <em>Compêndio de Teoria Elementar da Música</em> fala das propriedades do som musical e sua organização de forma condizente.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>A música do século XX apresentou diversos modelos teóricos que quebram com este entendimento simplificado sobre a música, suas unidades básicas e estruturas relacionadas<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Uma implementação não didática destas três linhas de código pode ser feita em uma só linha.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Duas iniciações científicas trataram do assunto.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>Também foi usado o nome FIGGS (FInite Group in Granular Synthesis) dado que o termo <em>unit synthesis</em> não é usual na literatura. Posteriormente o primeiro autor deste trabalho recorreu novamente ao uso do nome . Isso foi motivado pela utilidade da técnica para síntese de estruturas musicais, que, nos usos que o  teve, foi maior do que a utilidade para síntese de amálgamas sonoros tipicamente resultantes da síntese granular<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>Grãos sonoros e microsons são jargões equivalentes típicos da síntese granular. São usados para indicar sons com durações bastante curtas, como assinalado no texto.<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p>Para maiores detalhes, veja: <a href="http://toplap.org"><code class="url">http://toplap.org</code></a>.<a href="#fnref12">↩</a></p></li>
<li id="fn13"><p>Vide ’Manifesto Live coding’ em <a href="http://toplap.org/wiki/ManifestoDraft"><code class="url">http://toplap.org/wiki/ManifestoDraft</code></a>.<a href="#fnref13">↩</a></p></li>
<li id="fn14"><p>Disponíveis em <a href="http://vimeo.com/33012735"><code class="url">http://vimeo.com/33012735</code></a>, <a href="http://vimeo.com/33018740"><code class="url">http://vimeo.com/33018740</code></a>, <a href="http://vimeo.com/33019291"><code class="url">http://vimeo.com/33019291</code></a>, <a href="http://vimeo.com/33025717"><code class="url">http://vimeo.com/33025717</code></a> e <a href="http://vimeo.com/33025913"><code class="url">http://vimeo.com/33025913</code></a>.<a href="#fnref14">↩</a></p></li>
<li id="fn15"><p>Veja: <a href="http://automata.github.com/vivace"><code class="url">http://automata.github.com/vivace</code></a>.<a href="#fnref15">↩</a></p></li>
<li id="fn16"><p>Uma amostra está disponível <em>online</em> para ser utilizada por qualquer em qualquer computador com o navegador <em>Google-Chrome</em>: <a href="http://pulapirata.com/skills/vivace"><code class="url">http://pulapirata.com/skills/vivace</code></a>.<a href="#fnref16">↩</a></p></li>
<li id="fn17"><p>Veja o repositório: git://labmacambira.git.sourceforge.net/labmacambira/audioArt/ABT<a href="#fnref17">↩</a></p></li>
<li id="fn18"><p>Uma associação civil formada e desmembrada em 2008, sediada em Recife, PE.<a href="#fnref18">↩</a></p></li>
<li id="fn19"><p>Veja: <a href="http://pontaopad.me/origami-sensores"><code class="url">http://pontaopad.me/origami-sensores</code></a>.<a href="#fnref19">↩</a></p></li>
<li id="fn20"><p>Evento mensal que ocorre na cidade São Paulo: AudioVisual Ao Vivo.<a href="#fnref20">↩</a></p></li>
<li id="fn21"><p>Veja a página principal do espaço: <a href="https://www.assembla.com/spaces/audioexperiments"><code class="url">https://www.assembla.com/spaces/audioexperiments</code></a>.<a href="#fnref21">↩</a></p></li>
<li id="fn22"><p>Há uma plataforma aberta com capacidades de <em>wiki</em>, blog e midiateca em <a href="http://estudiolivre.org"><code class="url">http://estudiolivre.org</code></a>.<a href="#fnref22">↩</a></p></li>
<li id="fn23"><p>Renato Fabbri, Ricardo Fabbri, Vilson Vieira, Marcos Mendonça e Gilson Beck mentoraram; Alexandre &quot;Bzum&quot; Negrão, Lucas Zambianchi, Larissa Arruda, Nivaldo Henrique Bondança, Fernando Gorodscy, Andres Martano desenvolveram como bolsistas; Danilo Shiga e Marcos Murad foram parceiros colaboradores; Francisco Simões, Fábio Simões, Caleb Mascarenhas Luporini, &quot;Gera&quot; Magela, Edson &quot;Presto&quot; Correia, Daniel Penalva se aproximaram para atividades específicas, cruciais em todo ano de 2012 já sem os investimentos do convênio.<a href="#fnref23">↩</a></p></li>
<li id="fn24"><p><a href="http://estudiolivre.org/python-e-som-tutorial"><code class="url">http://estudiolivre.org/python-e-som-tutorial</code></a>.<a href="#fnref24">↩</a></p></li>
<li id="fn25"><p><a href="http://estudiolivre.org/tiki-index.php?page=Video+Tutoriais"><code class="url">http://estudiolivre.org/tiki-index.php?page=Video+Tutoriais</code></a>.<a href="#fnref25">↩</a></p></li>
<li id="fn26"><p>Veja <a href="http://labmacambira.git.sourceforge.net/git/gitweb.cgi?p=labmacambira/audioArt;"><code class="url">http://labmacambira.git.sourceforge.net/git/gitweb.cgi?p=labmacambira/audioArt;</code></a>.<a href="#fnref26">↩</a></p></li>
<li id="fn27"><p>Veja <a href="http://labmacambira.git.sourceforge.net/git/gitweb.cgi?p=labmacambira/lv2Tut;"><code class="url">http://labmacambira.git.sourceforge.net/git/gitweb.cgi?p=labmacambira/lv2Tut;</code></a>.<a href="#fnref27">↩</a></p></li>
<li id="fn28"><p>Veja: <a href="https://vimeo.com/channels/labmacambira."><code class="url">https://vimeo.com/channels/labmacambira.</code></a><a href="#fnref28">↩</a></p></li>
<li id="fn29"><p>Veja a página do PDF <a href="http://estudiolivre.org/tiki-index.php?page=fdp&amp;highlight=fdp"><code class="url">http://estudiolivre.org/tiki-index.php?page=fdp&amp;highlight=fdp</code></a>, o artigo aceito no LAC2008 <a href="http://www.estudiolivre.org/el-gallery_view.php?arquivoId=8221"><code class="url">http://www.estudiolivre.org/el-gallery_view.php?arquivoId=8221</code></a> e o repositório do FDP <a href="http://sourceforge.net/projects/fdpack/develop fdpsf"><code class="url">http://sourceforge.net/projects/fdpack/develop fdpsf</code></a>.<a href="#fnref29">↩</a></p></li>
<li id="fn30"><p>Veja: <a href="http://www.estudiolivre.org/carta-pts-midias-livres"><code class="url">http://www.estudiolivre.org/carta-pts-midias-livres</code></a><a href="#fnref30">↩</a></p></li>
<li id="fn31"><p>Veja: <a href="http://disqus.com/research/pseudonyms/"><code class="url">http://disqus.com/research/pseudonyms/</code></a><a href="#fnref31">↩</a></p></li>
<li id="fn32"><p>Veja: <a href="http://mutgamb.org/blog/Submidialogias-Peixe-Morto-para-Baixar"><code class="url">http://mutgamb.org/blog/Submidialogias-Peixe-Morto-para-Baixar</code></a> e <a href="http://culturadigital.br/contraculturadigital/2012/02/01/publicacao-contraculturadigital/"><code class="url">http://culturadigital.br/contraculturadigital/2012/02/01/publicacao-contraculturadigital/</code></a>.<a href="#fnref32">↩</a></p></li>
<li id="fn33"><p>O canal pode ser acessado em: <a href="http://vimeo.com/channels/labmacambira"><code class="url">http://vimeo.com/channels/labmacambira</code></a>.<a href="#fnref33">↩</a></p></li>
<li id="fn34"><p>Veja: <a href="https://vimeo.com/33012735"><code class="url">https://vimeo.com/33012735</code></a>, <a href="https://vimeo.com/33018740"><code class="url">https://vimeo.com/33018740</code></a>, <a href="https://vimeo.com/33019291"><code class="url">https://vimeo.com/33019291</code></a>, <a href="https://vimeo.com/33025717"><code class="url">https://vimeo.com/33025717</code></a>, <a href="https://vimeo.com/33025913"><code class="url">https://vimeo.com/33025913</code></a>.<a href="#fnref34">↩</a></p></li>
<li id="fn35"><p>Veja: <a href="vimeo.com/channels/labmacambira/2681879"><code class="url">vimeo.com/channels/labmacambira/2681879</code></a>.<a href="#fnref35">↩</a></p></li>
<li id="fn36"><p>Os arquivos fonte estão nos repositórios em <a href="http://labmacambira.git.sourceforge.net/git/gitweb-index.cgi"><code class="url">http://labmacambira.git.sourceforge.net/git/gitweb-index.cgi</code></a>.<a href="#fnref36">↩</a></p></li>
<li id="fn37"><p>Veja: <a href="http://wiki.nosdigitais.teia.org.br/Lab_Macambira"><code class="url">http://wiki.nosdigitais.teia.org.br/Lab_Macambira</code></a>, <a href="http://www.estudiolivre.org/el-user.php?view_user=gk"><code class="url">http://www.estudiolivre.org/el-user.php?view_user=gk</code></a>.<a href="#fnref37">↩</a></p></li>
<li id="fn38"><p>Veja: <a href="http://pontaopad.me/epads"><code class="url">http://pontaopad.me/epads</code></a><a href="#fnref38">↩</a></p></li>
<li id="fn39"><p>Veja: <a href="http://wiki.nosdigitais.teia.org.br/AA"><code class="url">http://wiki.nosdigitais.teia.org.br/AA</code></a>.<a href="#fnref39">↩</a></p></li>
</ol>
</div>
