\chapter{Música digital em domínios não digitais}
\label{cap:musicaExtra}

As tecnologias expostas neste trabalho tiveram origem em
usos reais, advindas
de práticas culturais e voltadas para repercussões sociais.
Este material humano e artístico é o assunto deste
capítulo.
Pode-se dividir estes resultados em:
experimentos abertos em áudio,
música em tempo real,
música em tempo diferido,
música na matéria e
repercussões no tecido social.
Estes tópicos são tratados separadamente
abaixo em termos do que é
mantidas \emph{online}.
Estas atividades extrapolam os usos musicais, especialmente devido às
finalidades pedagódicas e sociais dos materiais audiovisuais.
Assim, por completude, alguns materiais didáticos
e ferramentas web são descritas.

\section{Experimentos abertos em áudio: LADSPAs, Wavelets e Redes Complexas}

Todos os desenvolvimentos desta dissertação estão em repositórios
abertos.\cite{repoDissertacao}

O código computacional feito para a arte sonora, incluindo a musical,
 manifesta-se como cultura pois é fruto de práticas
espontâneas, diárias e coletivas.
\footnote{As chamadas culturas biopunk, ciberpunk, cipherpunk, hacker,
  digital e outras mais, possibilitadas pelos desenvolvimentos
  em telecomunicação, dizem respeito em menor ou maior grau à produção
  de código como cultura. As elaborações de conceitos e ferramentas
  voltados para o compartilhamento nestas culturas estão na gênese do que
  se entende como 'Cultura Livre'.}
A seguir estão alguns exemplos de resultados
destas incidências culturais,
especificamente em áudio
 e música.

\subsubsection{Plugins LADSPA e lv2}

LADSPA (Linux Audio Developers Simple Plugin API) é a API
livre de plugins de áudio até a presente data. A
última versão é a 1.1, em uso corrente até hoje e é
de 2002 segundo os arquivos de cabeçalho dos códigos relacionados.
O LADSPA \emph{version} 2, abreviado lv2, é uma segunda versão do
protocolo também estabelecida e estável, mas extensível, motivo pelo
qual está sempre em desenvolvimento.

Na síntese, para maior qualidade, 
pode-se fazer a recomposição a partir do espectro desejado, como na seção~\ref{subsec:ruidos}.
Menos purista porém mais eficiente é
usar uma amostra curta do ruído e reproduzi-la indefinidamente
através de \emph{cross-fades}. Esta eficiência é desejada em um plugin,
pois seus usos comuns estão aplicações em tempo real.

Através deste trabalho, foi disponibilizado um pacote de plugins lv2
que sintetizam os ruídos branco, azul, violeta, rosa e marrom. Os
  códigos em C e a implementação como plugin lv2 estão no
  repositório git como pode ser visto em \emph{online}\footnote{
      este link é o README do repositódio git com todos os códigos dos
      plugins:
  http://labmacambira.git.sourceforge.net/git/gitweb-index.cgi.}.

Já na remoção de ruídos as abordagens são as mais variadas e
extremamente dependentes da aplicação. A complexidade dos algorítmos
atinge níveis de especialidade em processamento de sinais que já
mereceram trabalhos dedicados. Adiante está a implementação de um
removedor de ruído \emph{'Hum'}. Este ruído é
geralemte causado pela corrente alternada que
alimenta os equipamentos utilizados.

A remoção de ruído \emph{Hum} é baseada em uma sequência de filtros nó rejeita
banda (veja subseção~\ref{subsec:filtros}
idealmente dispostos nos harmônicos da frequência de
oscilação da corrente elétrica. Cada um destes filtros deve permitir
ajustes finos no fator de qualidade e também na frequência central
pois trata-se de um sistema real passível dos mais diversos efeitos de
distorção do modelo ideal. O código C/C++ e a
implementação como plugin está no mesmo repositório disponibilizado
para a síntese de ruídos.


\subsubsection{\emph{Wavelets}}
Alguns dos experimentos em \emph{wavelets} praticados estão em
Audioexperiments.\cite{Audioexperiments} Uma destas investidas
superou em muito um experimento e constitui um protocolo de compactação de áudio desenvolvido com o Prof. Rafael Santos Mendes na FEEC/UNICAMP como consequência do mestrado realizado por André Luvizzoto sob orientação do mesmo professor.
O método está bem descrito no artigo desenvolvido. Em resumo, o método consiste particionar o áudio com sobreposições para contemplar aplicações reais sem as distorções de borda da decomposição e recomposição. Em seguida, o áudio é decomposto em árvore \emph{Wavelet Packet}. Cada folha é então ordenada, os coeficientes com baixa energia eliminadose um polinômio encontrado pelo método dos mínimos quadrados é usado para representar os coeficientes restantes. Detalhes de escrita e leitura do polinômio completam o protocolo. Aplicações para stream de voz em tempo real foram vislumbradas e testes adicionais propostos.\cite{FabbriWavelets}

\subsubsection{Redes Complexas}
Com o fim de detectar nuances emocionais no uso da linguagem natural, foram desenvidos alguns trabalhos em análise de fala e de texto, ambos focados no uso de redes complexas. Três trabalhos: dois de processamento de texto e um de fala. O primeiro trabalho de processamento de texto foi apresentado na ACL de 2010, em Upsala, na Suécia, pelo Prof. Thiago Pardo, atual presidente da ACL.\cite{FabbriACL,FabbriComplenetVoz,FabbriComplenetTexto}

Os trabalhos focaram em distinguir polaridade em excertos de textos e de fala. Nos textos, a rede é formada a partir da sequência de palavras, em que cada palavra é um vértice e a sucessão imediata cria uma aresta. O pré-processamento dos textos consiste na lematização e na retiradas das \emph{stop-words} e da pontuação. Nas falas, as bandas de frequência utilizadas são vértices e a transição entre uma banda e outra é uma aresta. Ambas redes com peso e podem ser utilizadas como direcionais ou não. Os trabalhos consistiam em formar estas redes com os bancos de dados correlatos, extrair medidas e aplicar reconhecimento de padrão. Ambos os estudos não fazem uso do conteúdo semântico, mas sim topológico dos sinais recebidos, e chegam a mais 80\% de cobertura ou precisão em alguns casos, podendo contribuir com métodos tradicionais que fazem uso de redes semânticas, por exemplo.

\section{Áudio e Música}

Este capítulo está dividido em 4 partes: música em
tempo diferido (ou seja, que não é feito em tempo real), música em
tempo real, música na matéria (suporte físico em \emph{hardware}) e música no
tecido social (considerando especialmente as mobilizações humanas
relacionadas).

Com isso desejamos expor uma incidência da prática musical através do código com
exemplos reais de aplicação e em utilização pelo autor, membros do
LabMacambira.sf.net, parceiros, colaboradores e por usuários
eventuais das naturezas mais diversas.


\subsection{Música em Tempo Diferido: Minimum-fi e FIGGUS}

\begin{quotation}
\small 'I also find that intelligent people always respect the intelligence needed to construct a simple structure in a clear way that really works.'

\emph{Tom Johnson}
\end{quotation}

A realização musical em tempo diferido é o paradigma inicial da música
computacional, iniciando com o Music V, depois
com o CSound. Pode-se dizer que até hoje é a forma como
compositores usualmente pensam a música: concebendo e escrevendo as
estruturas que depois são executadas por instrumentistas ou aparelhos
eletrônicos.

Assim como a composição instrumental permite alguns refinamentos estruturais não presentes na improvisação instrumental,
a realização musical em
tempo diferido permite um detalhamento maior dos
procedimentos do que a realização em tempo real. Por este mesmo
motivo.

O som musical pode ser caracterizado fisicamente, tornando possível a  sua síntese
digital. Estes sons e a organização deles são objeto do Capítulo 2 da presente dissertação.
A utilização destes sons diretamente, utilizando somente os recursos básicos de justaposição e sobreposição, está na pequena peça \emph{minimum-fi}. Já o \figgus\ (FInite Groups in Granular and Unit
Synthesis), utiliza os princípios do minimum-fi e utiliza simetrias, através de
permutações e de Teoria de Grupos, para a composição de músicas.
O \figgus\ gera um
EP\footnote{\emph{Extended Play}, um álbum musical maior que um single mas
  menor que um LP (Long Play) inteiro.} com um único comando. Este é o
\emph{PPEPPS}\footnote{Pure Python EP: Projeto Solvente}, como veremos
a seguir.

Desta forma, esta sessão exemplifica e explicita - através de dois
exemplos reais - o uso de código para a síntese
\emph{musical}, desde as amostras relativas a uma nota com dada
frequencia, amplitude e timbre, até a confecção de uma ferramenta
derivada, já incorporando propostas musicais e estruturas mais
elaboradas.

\subsubsection{\emph{Minimum-fi}}

Existe uma perene matiz estética e também tecnológica dedicada a realizar uma
dada tarefa com \emph{o mínimo} necessário.  Na música, esta matiz é
igualmente presente e decorre em grande parte do princípio de unidade
e coerência\footnote{Este princípio tanto é fundamental que as escolas
  musicais possuem técnicas específicas, músicas possuem suas próprias
  convenções mantidas por toda a sua duração, os arcos mantém
  características, enfim, podemos até mesmo concluir que as simetrias
  definem o escopo musical.}. 
  Em código computacional, a empreitada para
manifestar este princípio ele próprio de forma mínima resultou no
\emph{minimum-fi.py}, código Python em um único arquivo curto que
sintetiza a música segundo as estruturas especificadas em
linha. Na versão atual, de
2012 mas adiantada em 2011, os algorítmos em Python propriamente ditos
somam 53 linhas e incluem apenas 5 funções. Com estas funções,
estruturas musicais são criadas padrão a padrão, nota a nota,
amostra por amostra. Na prática, e em linguagem cotidiana,
as notas formam blocos e estruturas hierarquicamente
superiores.

Os princípios, bastante simples\footnote{Osvaldo Lacerda, em seu livro
  \emph{Compêndio de Teoria Elementar da Música} fala das propriedades
  do som musical e sua organização de forma condizente.}, são:
\begin{itemize}
  \item Deve-se ter um mecanismo de síntese sonora que possibilite a
    geração de unidades sonoras com diferentes timbres, controle sobre
    a frequência fundamental, duração e volume, como especificado na seção~\ref{sec:notasDisc}.
  \item Deve-se ser capaz de construir
    séries de unidades, sejam sobrepostas (e.g. acordes)
    ou justapostas (e.g. melodias).
\end{itemize}

Para o primeiro item, se prestam os procedimentos de busca
em tabelas/vetores com formas de ondas em alta resolução, chamado
\emph{lookup table}. O procedimento é barato computacionalmente,
com resultados diversificados e tidos como de alta qualidade
pois não acrescentam ruídos relevantes ao sinal. Uma descrição
do procedimento está na subseção~\ref{subsec:lookup}.

Através da utilização do lookup sucessivo (procura-se um valor na primeira tabela
e este valor indica o valor na segunda tabela a ser utilizado), executa-se
um \emph{waveshaping}. Este procedimento é bastante apreciado pela simplicidade e eficácia
na síntese de tímbres diversos e ricos em harmônicos e evolução temporal. Embora
uma explicação exaustiva do waveshaping fuja ao escopo deste trabalho, este
método se caracteriza pela aplicação de uma função não linear ao sinal de entrada.
Neste caso o sinal de entrada é gerado pela primeira busca, a função não linear aplicada
é a segunda busca, na outra tabela.

Existem muitas formas de se executar waveshaping, a seguir segue o que usamos no minimum-fi, que se sustenta principalmente por ser leve e simples:

\code{Waveshaping com consultas sucessivas a tabelas}{python_snippets/lookup_cruz.py}

O segundo item - dos dois princípios expostos sobre o minimum-fi - presta-se à discretização do espaço musical.
Unidades como batidas e notas
tornam mais eficiente a comunicação pois a quantidade
de estruturas sugeridas é maior e as estruturas são mais claras no discreto do que no contínuo.\cite{Roederer} Roederer chega a
apontar que as próprias notas dos instrumentos musicais são um reflexo de que é mais eficiente
o uso do discreto do que do contínuo para a geração de estruturas musicais.

De fato, unidades bem definidas se mostram úteis na prática musical 
para fazer sequências de unidades, concatená-las no tempo. Quando as unidades
são notas com frequência definida, as sequências de unidades justapostas no tempo tendem a ser compreendidas como melodias ou linhas melódicas. As
sequências sobrepostas no tempo são comumente compreendidas como acordes, mas podem ser tidas simplesmente
como sobreposições circunstanciais de duas ou mais linhas melódicas\footnote{A música do século XX apresentou diversos modelos teóricos que quebram com este entendimento simplificado sobre a música, suas unidades básicas e estruturas relacionadas}.\cite{Lacerda}

As duas construções básicas explicitadas, baseadas na dicotomia melodia/harmonia - relacionada à horizontalidade/verticalidade, justaposição/sobreposição - são
as funções \emph{fazSequencia} e \emph{fazAcorde} no minimum-fi. Vale notar elas são absolutamente 
equivalentes em uma análise puramente conceitual, i.e. uma delas pode ser omitida sem perda das possibilidades musicais. Isso fica particularmente óbvio quando se nota que os procedimentos de mixagem e concatenação são
plenamente capazes de realizar o que estas funções realizam. Aliás, as funções nada
mais são do que usos típicos e quase caricatos destes procedimentos: no \emph{fazAcorde} a mixagem
sobrepõe no tempo todas as unidades, no \emph{fazSequencia} as unidades são todas juntapostas no tempo.

Como pode-se notar a seguir, as sequências de notas e os acordes, em última instância, são utilizações específicas das 2 funções de síntese sonora explicadas anteriormente: lookup e lookupcruz.

\code{Realização de Sequências de Notas}{python_snippets/fazSequencia.py}

\code{Realização de Acordes de Notas}{python_snippets/fazAcorde.py}

A última das cinco funções utilizadas é uma soma amostra a amostra de dois sons. Para isso,
é necessário completar com zeros a sequência com o menor número de amostras para somar rapidamente:

\code{Somador (função auxiliar)}{python_snippets/somador.py}

Com isso o básico está coberto e o foco vai para a criação de estruturas. Por exemplo: 
pode-se criar as
escalas completamente simétricas na oitava cromática, escalas diatônicas ou até microtonais, como as descritas na subseção~\ref{subsec:intervalos}.

O estabelecimento de pequenas sequências é bastante útil para
reutilizações, variações e geração de materiais derivados, por exemplo: 
\code{Sequências diversas}{python_snippets/series.py}

O passo seguinte é sintetizar e mixar para a obtenção 
de sequências musicais. A síntese de sequências e acordes
podem ser feitos desta forma:

\code{Sintetizando sequências e acordes}{python_snippets/seqs_acordes.py}

Já a síntese de estruturas compostas (e.g. sequências de acordes e sobreposição
de linhas melódicas), é feita com os recursos usuais da liguagem:
listas em Python e vetores Numpy (mais eficientes em tempo de
execussão e também na simplicidade do código). Estes mesmos
procedimentos são praticamente os mesmos em Scilab, C/C++, Javascript, PHP, etc.

A seguir, para fins didáticos, está a construção de acordes periódicos em
python puro\footnote{Uma implementação não didática destas três linhas de código
pode ser feita em uma só linha.}:

\code{Acordes periódicos}{python_snippets/acordes_periodicos.py}

Em posse destas sequências, acordes e recursos da linguagem,
são formadas estruturas hierarquicamente superiores
através da concatenação de estruturas, da mixagem de estruturas, e da amplificação
(ou atenuação) seletiva das mesmas:

\code{Amplificação e mixagem}{python_snippets/amp_mix.py}

Neste ponto, basta criar músicas e sequências de interesse estético ou para pesquisa. Os encadeamentos dependem de intensões estéticas, entendimentos musicais e estruturas abstratas que mantém a coerência e o interesse em uma peça musical. Para o leitor mais interessado, recomendamos uma visita ao capítulo~\ref{cap:resultados} e à \emph{toolbox} \massa, onde pode-se encontrar o próprio script \emph{minimum-fi.py} e outros experimentos. 
A seguir utilizamos esta base apresentada para sintetizar estruturas musicais e então um EP.

\vspace{10 mm}

\subsubsection{FIGGUS: FInite Groups in Granular and Unit Synthesis}

Nesta subseção há um foco especial nas estruturas musicais.
São elas, inseridas em um momento histórico e
executadas por instrumentos específicos com as técnicas de época,
que constituem em grande parte uma linguagem musical e músicas propriamente
ditas. O \figgus\ constitui uma técnica composicional
manifesta em software como ferramenta de síntese de
estruturas musicais. Esta técnica
consiste na utilização de estruturas matemáticas
para a representação de simetrias.

O \figgus\ foi iniciado em 2006 com o físico-matemático Prof. Adolfo Maia Junior (do IMECC e NICS, ambos da UNICAMP) - bem anterior
ao nascimento do \emph{minimum-fi} - para
tratar de simetrias na música com vistas à composição musical através
de métodos matemáticos\footnote{Duas iniciações científicas trataram do assunto.}. Mais especificamente, a proposta resultou em
um programa voltado para a síntese
granular e síntese de estruturas musicais através de Grupos Algébricos. O nome dado
foi \figgus, sigla de FInite Groups in Granular and Unit Synthesis\footnote{Também foi usado
o nome FIGGS (FInite Group in Granular Synthesis) dado que o termo \emph{unit synthesis} não
é usual na literatura. Posteriormente o primeiro autor deste trabalho recorreu novamente
ao uso do nome \figgus. Isso foi motivado pelo
uso da técnica para síntese de estruturas musicais, não para
síntese de amálgamas sonoros (ou timbres mesmo) tipicamente resultantes da síntese granular}.

Na atual reescrita, embora ainda bastante atrás do \figgus\ original quanto
à interface gráfica, a ferramenta opera diretamente em Python puro,
com as biblitecas imbutidas por padrão. Isso permite com que o FIGGUS
sintetize todo um EP usando somente os comandos:

\code{Utilizando o FIGGUS para Sintetizar um EP}{python_snippets/synth_ep.py}

Desta forma, a ferramenta 
é simples de ser usada para experimentações
e implementações adicionais.
Outro uso planejado para o \figgus\
é a síntese de tímbres e amálgamas sonoros através da
Síntese Granular. Embora o foco atual seja outro, cabe algumas breves
palavras sobre o assunto.

A Síntese Granular é uma área bem estabelecida tanto na acústica quanto
na Computação Musical e se caracteriza pela geração de sons bastante curtos
e em quantidade massiva. Tipicamente, os sons possuem entre 5 e 40 milissegundos
e a quantidade destes \emph{microsons}\footnote{Grãos sonoros e microsons são jargões equivalentes
típicos da síntese granular. São usados para indicar sons com durações
bastante curtas, como assinalado no texto.} pode chegar a milhares por segundo. O tratamento específico da
síntese granular foge ao escopo deste trabalho. O leitor interessado pode consultar os artigos produzidos sobre Síntese Granular e Teoria de Grupos.\cite{figgusOriginal,figgusEspacializacao}
Desta forma, o texto a seguir concentra-se em Grupos Finitos para a síntese de estruturas musicais.

Nas artes é de comum conhecimento o papel absolutamente central
que as simetrias possuem. Na música, para citar somente alguns exemplos simples,
há os numerosos estudos de simetrias na música de J. S. Bach, os jogos
de dados de Mozart e os usos recorrentes da proporção áurea na música
de Béla Bártok. Matematicamente, as \emph{simetrias} são descritas por Grupos,
e estes são definidos como um conjunto (seja $G$)
munido de uma operação (seja $\bullet$), formando um grupo $(G,\bullet)$
satisfazendo as propriedades descritas na subseção~\ref{estCic}.

No \figgus, o grão ou unidade sonora
é uma classe que possui apenas
os seguintes atributos: duração (segundos),
frequência (Hz), timbre (identificador para usar mediante implementações convenientes), intensidade (pico $\in \ [0,1]$), e duração dos fades (in e out em segundos).

%\code{Grão Sonoro Básico}{python_snippets/fgrao.py}

O \figgus\ funciona com base em uma sequência de grãos especificada inicialmente e na qual operam as permutações.

%\code{Sequência de Grãos}{python_snippets/fsequencia.py}

Aos grãos em sequência são aplicadas permutações.
Para isso é bastante conveniente representar as permutações em classes próprias.
A classe do padrão de permutação possui também um período de aplicação da permutação, ou seja, de quantas em quantas leituras da sequência apermutação é aplicada.

%\code{Permutações e Padrões de Permutações}{python_snippets/fpermutacoes.py}

De posse dos grãos, da sequência, das permutações e do padrão de permutações, pode-se realizar a estrutura musical em si. Basta adicionalmente especificar o número desejado de iterações da sequência. Com isso, a sequência de grãos é lida um número de vezes, aplicando as permutações na sequência de grãos segundo o padrão especificado, de forma a resultar em uma sequência musical. Note que se a permutação usar menos elementos que a sequência possui, alguns destes elementos ficarão estáticos nas iterações da sequência no padrão sonoro.

%\code{Realização do Padrão Musical}{python_snippets/fpadraosonoro.py}

Em posse não apenas de representações abstratas do padrão musical a ser realizado, mas dos próprios vetores sonoros relacionados à representação digital da música a ser realizada, pode-se escrever um arquivo de áudio propriamente dito. O mais conveniente neste caso é escrever um arquivo PCM (Pulse Code Modulation) em algum padrão amplamente utilizado e reconhecido. Ambos WAV e AIFF satisfazem estes requisitos. Mais especificamente, o padrão de CDs é WAV com 44100 amostras por segundo e 16 bits por amostra. As amostras dos vetores sonoros são calculados, por conveniência e convenção, no âmbito $[-1,1]$ e precisam ser normalizados para o âmbito $[-32767,32768]$ e truncados em números inteiros. Depois disso devem ser escritos em um arquivo com os \emph{bits} como na convenção da linguagem C/C++. A bibioteca \emph{struct} cuida dessa escrita do inteiro no formato correto, e a biblioteca \emph{wave} escreve o cabeçalho no formato WAV adequado. Assim, a clase de escrita do vetor sonoro em arquivo comum fica assim:

\code{Escrita do Vetor Sonoro em Arquivo WAV}{python_snippets/fio.py}

%%%%%%%%%%%%%%%%%% FIM DO PULO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Música em Tempo Real: Live coding e ABeatTracker (ABT)}

Com os avanços computacionais recentes, tornou-se usual a síntese
sonora em tempo real. Com isso, surgiram linguagens chamadas de
domínio específico, para o áudio e a música, em sua maioria dedicadas
-- ou ao menos capacitadas -- para o uso em tempo de
execução. Puredata, SuperCollider, ChucK e ixilang são exemplos de
linguagens largamente utilizadas para a composição musical e síntese
sonora em tempo real. Em outras palavras, estas linguagens
possibilitam que o usuário ouça o resultante sonoro do código
utilizado e altere o código com resultado imediato no processamento e
resultado que escuta.

Nesta linha de exploração musical do código, iremos expor a seguir
sobre nossas investidas em \emph{Live coding} (escrita de código em
tempo real com vistas à performance pública) através de linguagens em
contínuo desenvolvimento como ChucK, e o ABeatTracker (uma linguagem
por macros para execução sonora rítmica em conjunto com instrumentos
tradicionais e outras fontes sonoras/musicais externas).

\subsubsection{Live coding}

Recentemente, grupos de ponta em música experimental no mundo todo
estão desenvolvendo apresentações musicais públicas baseadas na
escrita de código ao vivo. Destes vale ressaltar os grupos pioneiros
slub e Benoît and the Mandelbrots e as ``orquestras de laptops''
PLOrk, SLOrk e DubLOrk\footnote{Para maiores detalhes vide
  \url{http://toplap.org}}. Usualmente, se projeta o código para que a
audiência possa ver o que está sendo escrito, no rítmo em que se
escreve, e se projeta também o resultante sonoro por autofalantes.

As motivações para isso são variadas. Expomos a seguir de forma
topificada um condensado do que nos orientou a tal prática. Vale
ressaltar que são motivações igualmente presentes em outros
grupos\footnote{Vide ``Manifesto Live coding'' em
  \url{http://toplap.org/wiki/ManifestoDraft}}, embora não
necessariamente em todos ou da mesmícima forma.

\begin{itemize}
    \item A performance musical por computador carece de recursos
      performáticos à altura das execuções com instrumentos
      tradicionais. Os gestos são por demais discretos e a
      concentração do performer é bastante focada na tela do
      computador.
    \item O feedback auditivo do código projetado permite que o
      espectador infira significados dos códigos. Mesmo que de forma
      superficial, este recurso do Live coding é em muito capaz de
      desmistificar a programação de computadores e sua aplicação em
      computação musical, por muito considerada completamente
      intangível.
    \item O código em si é um recurso poderosíssimo, que permite ao
      usuário controlar os sons produzidos amostra por amostra ou em
      escalas maiores, como notas, compassos, fraseados inteiros ou
      mesmo em escalas maiores de tempo, como minutos, horas, dias e
      semanas.
    \item O compartilhamento do código é usual, leve e eficiente como
      entrega completamente aberta da tecnologia relacionada à
      proposta estética. Isso motiva não só o programador à aplicar
      seus conhecimentos na música, mas também o músico a adentrar o
      uso de linguagens de programação para expressão de suas ideias
      musicais.
\end{itemize}

Desta forma, iniciamos em 2011 uma linha de atuação em Live coding com
a criação do duo \emph{FooBarBaz} (Variáveis Metasintáticas) composto
pelo autor Renato Fabbri e Vilson Vieira. Essa atuação resultou em uma
performance no \emph{V Festival Contato}. É interessante ressaltar que
essa foi a primeira apresentação de Live coding no Brasil, e a maior
apresentação internacional em número de ouvintes (por volta de 5 mil
pessoas) que se tenha relato.

Utilizamos a linguagem ChucK por apresentar os recursos que
consideramos mais apropriados, embora de forma alguma isso seja
consensual na prática atual de Live coding. Ainda, a apresentação
contou com recursos adicionais para agregar interesse, como a
utilização do \emph{cowsay} (para enviar mensagens enquanto se
desenrolava a música) e trajetórias de um ponto no fundo do código
para sugerir a vertigem do sono REM. Estes recursos podem ser vistos
em uso claramente nos videos demonstrativos\footnote{Disponíveis em
  \url{http://vimeo.com/33012735}, \url{http://vimeo.com/33018740},
  \url{http://vimeo.com/33019291}, \url{http://vimeo.com/33025717} e
  \url{http://vimeo.com/33025913}.}  As linhas do \emph{cowsay} e o
\emph{script} em linguagem Processing relacionados, assim como maiores
detalhes sobre o duo e fotos da apresentação podem ser vistos em
\url{http://wiki.nosdigitais.teia.org.br/FooBarBaz}.

Duas pessoas executaram Live coding simultaneamente. A saber, Vilson
Vieira executava ritmos, batidas bastante marcadas que serviam como
base. Renato Fabbri, autor do presente trabalho, executava linhas
fluídas quasi melódicas que formavam arcos maiores. Nos intervalos das
investidas no formato citado, havia interlúdios em que ambos se
revesavam com músicas curtas e inusitadas, como em um duelo. A mixagem
e espacialização dos dois canais de áudio foram controladas por um
patch em Puredata que permitia o \textit{cross-fading} entre os canais
através da detecção do movimento das mãos. Isso foi possível através
do objeto em Puredata/GEM chamado \emph{color classify}, criado por
Ricardo Fabbri e posto em uso por Gilson Beck durante a
apresentação. Esse mesmo objeto foi utilizado no instrumento
AirHackTable, discutido na seção~\ref{aht}.

A prática de Live coding requer o uso de linguagens que permitam a
execução rápida de instruções curtas, incentivando assim o improviso
musical através do código. Com esse objetivo, ambos \emph{live coders}
usaram editores de texto -- vim e Emacs -- que permitiam a comunicação
imediata com a linguagem de síntese sonora ChucK.  Acreditando que a
música, especialmente sua vertente experimental, se baseia fortemente
na organização de trechos de arquivos de áudio ou notas dadas a
instrumentos sintetizados, foram gerados códigos que permitissem tal
manipulação. Renato utilizou de trechos (``esqueletos'') minimalistas
de código em ChucK para reproduzir arcos longos e estruturas
melódicas, como os que seguem:

\code{Interface para controle de parâmetros em tempo
  real}{chuck_snippets/bar.ck}

Ao mesmo tempo, Vilson utilizou dos códigos seguintes, também em
ChucK, que permitem facilmente especificar a posição do arquivo de
áudio que deve ser executada, sua velocidade de execução, duração e
amplitude. Uma interface minimalista foi utilizada para permitir a
alteração rápida desses parâmetros em tempo de execução.

\code{Interface para controle de parâmetros em tempo
  real}{chuck_snippets/foo.ck}

\code{Classe para \textit{sampling} de arquivos de
  áudio}{chuck_snippets/foosp.ck}

Esses desenvolvimentos todos inspiraram a criação de uma linguagem
específica para Live coding, chamada Vivace, que está em constante
desenvolvimento e tem seu código e maiores detalhes disponíveis em
\url{http://automata.github.com/vivace}. Já foram realizadas novas
apresentações utilizando essa linguagem, a notar: Hacklab do Velho, em
São Carlos (SP), Semana da Comunicação FAAP (SP), Palco UFSCar (SP) e
Semana Nacional da Ciência e Tecnologia (SP). Nelas houve a
predominância do apelo às mídias populares, em uma reinauguração do
gênero da \emph{pop art}: trechos de vídeos de uma novela brasileira
eram rearanjadas em tempo real, contrastando com linhas rítmicas e
melódicas executadas em sincronia com o material visual\footnote{Uma
  amostra está disponível on-line para ser utilizada por qualquer
  pessoa que possua um navegador Chrome:
  \url{http://pulapirata.com/skills/vivace}}.

\subsubsection{ABeatTracker (ABT)}

O ABT é uma linguagem/ferramenta que dispara linhas rítmicas através
de macros que especificam as células rítmicas, amostras sonoras que
são utilizadas como conteúdo sonoro destas linhas, modos de leitura
destas amostras sonoras, e variáveis randômicas utilizadas para
execussão da linha. Além disso, o ABT dispõe de variáveis globais que
podem ser alteradas a qualquer momento pelo usuário, como BPM,
velocidade de leitura das amostras e variáveis randômicas globais (que
se somam às individuais).

Em pouco explicitaremos a forma de utilização do ABT. De antemão, é
pertinente apresentar uma pequena discussão a respeito do que o ABT é
considerado e sobre os propósitos desta ferramenta. Em primeiro lugar,
vale manter em mente que o ABT tem um funcionamento específico que
pode transcender o que se conhece por Live coding. As macros
pré-estabelecidas engendram um conjunto de recursos pré-estabelecidos
bem definido, o que contrasta com a ideia de uma 'linguagem de
programação' que tenha capacidades mais amplas. De qualquer forma,
linguagens com domínios específicos não são raras e por vezes o ABT
foi descrito como uma linguagem.

Sobre os propósitos do ABT, em primeiro lugar ele se dispõe a ser um
instrumento computacional essencialmente rítmico. Junto a esta
proposta, vem a necessidade da utilização em conjunto com outros
instrumentos, externos ao ABT, ao computador em que estiver executando
e possivelmente externo com relação a qualquer computador. Para isso
foi elaborado o ABD (ABeatDetector), no qual o usuário tamborila no
teclado do computador os rítmos que estiver ouvindo ou imaginando para
que o ABT sincronize o pulso e utilize células rítmicas
relacionadas. A análise feita pelo ABD resulta em uma série de ritmos
explicitados por sequências de compassos que encapsulem durações
regulares do ritmo tamborilado. Estes ritmos relacionados ao
tamborilar do usuário são chamados de harmônicos e podem ser
selecionados prontamente para o disparo de linhas melódicas.

No Apêndice XX está o manual de utilização do ABT e todo o código do
ABT (e do ABD) está disponivel online (citar repos). Exploramos abaixo
detalhes relevantes da implementação, em especial a forma de
funcionamento do ABD.

[códigos-chave e explicações sobre o ABT e o ABD]

\subsection{Música na Matéria: EKP e AHT}
\label{aht}

Embora o foco deste trabalho seja na exploração musical através de
códigos (abertos!), nesta sessão iniciamos uma explicação simples,
clara e factual de como estas investidas transcendem o código e até
mesmo a música em si.

Mais relevantes que os desenvolvimentos em si, são as mobilizações
criadas nos entornos e os engajamentos. Isso ficará evidente nas
próximas - curtas - sessões destes desenvolvimentos e resultados.

Aqui apresentamos dois trabalhos que geraram alguma movimentação de
pessoas, resultando em reuniões, desenvolvimentos, pesquisas e
apresentações propriamente ditas. O primeiro utiliza o estado do
hardware como entrada, o segundo se trata de uma mesa escultural para
flutuação de origamis que resulta em um instrumento musical bastante
lúdico.


\subsubsection{Emotional Kernel Panic (EKP)}

Em 2008, colaborando intensamente com o CDTL (Centro de
Desenvolvimento de Tecnologias Livres)\footnote{foi uma associação
  civil formada e desmembrada em 2008 e sediada em Recife, PE} foi
lançada a ideia de utilizar o estado do sistema operacional -
especialmente o kernel linux - para geração de sons. Surge o Emotional
Kernel Panic (EKP) na colaboração excepcional de Felipe
Machado\footnote{Importantíssimo para o desenvolvimento da Cultura
  Digital no Brasil [citar fontes e programas governamentais]},
Ricardo Brazileiro\footnote{Artivista Digital bastante ativo [citar
    fontes]} e o primeiro autor do presente trabalho.

Desde o inicio, foram definidos três finalidades para esta exploração
do SO:

[visitar o README do EKP e achar os scripts do Brazileiro sobre o EKP]

\begin{itemize}
\item Didáticos
\item Artísticos
\item Monitoramento do SO
\end{itemize}

Toda a base do EKP está aqui: http://trac.assembla.com/audioexperiments/browser/ekp-base

[Patches desenvolvidos por Brazileiro/Machado e rascunho do EKP-Monitor]

\subsubsection{AirHackTable}

A AirHackTable (AHT) é um instrumento musical eletrônico controlado por
origamis (dobraduras de papel), construída na forma de uma mesa. Nela,
uma rede de coolers reciclados faz flutuar origamis de geometria e
cores variadas. Os movimentos dos origamis são captados por webcam e
interpretados em tempo real por software de processamento de imagens,
gerando padrões que controlam a transformação sonora da música. Dessa
forma, pode-se dizer que os sons gerados refletem o voo dos origamis
de acordo com suas geometrias (que geram trajetórias de voo
caracteristicas).

Na versão atual da AirHacktable, cada cor (vermelho, amarelo, azul,
verde, preto, ou branco) controla uma voz, e a posição do origami na
mesa modula aspectos do som. Em outras palavras, se o origami está
flutuando mais à esquerda, o som sai no canal à esquerda da mesa, e se
o origami está mais próximo da câmera, o volume aumenta, e se está
mais afastado do operador, o som se torna mais agudo.

A AHT segue a filosofia de desenvolvimento contínuo, comum ao software
livre, que atualmente foi incorporado ao movimento chamado hardware
livre ou aberto.

[citar apresentações no contato, avav, oficinas sesc, chico, oficina semana imagem e som]

[imagens da AHT e dos patches em operação, ver com renato quais ele quer]

\subsection{Música no Tecido Social: Sabrina Kawahara, Audioexperiments, EstudioLivre.org, CDTL, juntaDados.org, Devolts.org, MSST, LabMacambira.sf.net}

Esta é uma sessão menor, dedicada a apontar repercussões emergentes
destas empreitadas em comunidades diferentes e então tornar
compreensível o desdobramento que estes códigos dedicados à música
tiveram em processos sociais e mobilizações civis.

Grupos foram montados em torno destes desenvolvimentos. Como os
propósitos de compartilhamento e apropriação tecnológica estavam no
cerne dos grupos relacionados, as investidas naturalmente tomaram
teores engajados socialmente. A questão do empoderamento das pontas e
da criação de um patrimônio tecnológico da humanidade é consequência
quase imediata das posturas de compartilhamento e apropriação citados.

Discorremos brevemente sobre cada uma das iniciativas citadas:

\begin{itemize}
    \item Sabrina Kawahara
    \item Audioexperiments
    \item Estudiolivre.org
    \item CDTL
    \item JuntaDados.org
    \item Devolts.org
    \item MSST
    \item LabMacambira.sf.net
    \item Outros relacionados: MuSa, Metareciclagem, Submidialogia, Tainã
\end{itemize}

\section{Web}

  Difusão de informação com ênfase na facilitação
  da apropriação de tecnologias e de instancias políticas.

   \subsection{Tecnologias sociais de alta demanda: Sitios, Conteúdos e Articulação}

      \subsubsection{Sítios}

      FDDCA

      Ferramenta de comunicação

      (Cadastro dos pontos?)

      AA, SOS, Catalogo de Ideias, etc

      Meu site pessoal


      \subsubsection{Conteúdos}

      Wiki?

      \subsubsection{Articulação}

      IRC, Emails

\subsection{Disponibilização e desenvolvimento conjunto: wikis, etherpads, AA, Trac, IRC ..}

\subsubsection{Wiki}

\subsubsection{Trac}

\subsubsection{Screencasts - Vimeo}

\subsubsection{AA}

\subsubsection{Audio Experiments (Æ)}

\subsubsection{IRC}

\subsubsection{Etherpads}

\subsubsection{Outras fontes}

\section{Materiais didáticos}

  \subsection{Tutoriais em texto e código: Filtros, Nyquist e plugins LADSPA}

Os vários materiais didáticos produzidos constam no apêndice
deste trabalho. Um único destes será exposto a seguir por sua
capacidade de agregar os conteúdos dos capítulos anterioes:
os tutoriais de filtros e amostragem.

\begin{itemize}
    \item {\bf Tutorial de python para áudio e som}

Este tutorial foi levado para Berlim no LAC 2007 e sofreu melhoras desde entao. Esta
primeira versao ficou resumida em forma de texto no EL\footnote{http://estudiolivre.org/python-e-som-tutorial}. Em 2010
a Associacao Python Brasil escolheu este trabalho, então já mais amadurecido, para ser apresentado no
FISL em Porto Alegre. Como consequencia, foi feita uma série de video-tutoriais bastante utilizados\footnote{http://estudiolivre.org/tiki-index.php?page=Video+Tutoriais}.
Este tutorial foi comentado em listas em que o autor não participa (e outras em que o autor participa).

    \item {\bf Tutoriais de filtros e amostragem via python}

Voltados para explicitar principios fundamentais de áudio, estes tutoriais
são baseados código Python e o equivalente em C. Pequenas explicações são
dadas com o intuito de orientar a exploração inteligente destes \emph{snippets}.

\emph{Teorema de Amostragem}: estes scripts visam a experimentacao inteligente com
o Teorema de Nyquist. (descricao)

\emph{Filtros}: alem da explicitação sobre as diferenças entre filtros FIR e IIR,
duas utilizações clássicas destes filtros estão implementadas: Wavelets (FIR) e Quad (IIR)

Estes códigos podem ser baixados no repositório SVN do AudioExperiments. E os textos estão
na wiki (nos digitais ou EL, recriar pois os CDTL foram apagados)

    \item {\bf Tutorial de plugins lv2}

Dadas as dificuldades que o desenvolvimento dos \emph{plugins} de áudio apresenta,
desenvolvi um tutorial passo a passo com plugins que rodam em todas as etapas.
Ele é baseado em uma interface C++ para este padrão de plugin que eh implementado
em C. Os códigos e os textos estão todos em repositório.

    \item {\bf Microtutoriais Django ~\cite{dmicrotuts}}

Estes 'microtutoriais' são baseados nos conceitos de \emph{scripts mínimos} e
\emph{alterações puntuais}. O primeiro conjunto de microtutoriais é dedicado
a reconstruir o tutorial oficial do django de forma condensada e não prolixa.
O segundo destes conjuntos é dedicado a instrumentalizar de fato o leitor com
o entendimento do funcionamento dos princípios fundamentais deste framework.

    \item {\bf Philosometrics}

Embora este não seja um trabalho didático propriamente dito, ele tem este intuito
no cerne de sua concepção e surgimento. Em decorrência dele, surgiu o  Musimetrics,
o Cinemetrics e o Literametrics. Além disso, ele é um belo exemplo da
utilização das ciências duras para a análise de ciências humanas e foi acolhido
como tal em alguns momentos.

    \item {\bf Carta mídias livres}

Texto criado em decorrência da participação da comissão de seleção no
'Prêmio Mídias Livres', a convite do Ministério da Cultura por 'notório saber'.
Esta carta é um documento único no seu conteúdo, deixando às claras
o conceito de Mídias Livres como mídias não aprisionadas pelo conceito
de propriedade, ou seja, que priorizam a sua livre circulação e a possibilidade
de geração de materiais derivados, assim como sua geração aberta ao colaborativo e comunitário.

    \item {\bf Textos de cunho sociológico, transformador}

Produção mais numerosa que as anteriores, se caracteriza por métodos não convencionais
de abordagem dos assuntos e de escrita. Em especial utiliza-se pseudônimos para
auxiliar a despersonificação, gerando textos menos presos à satisfação da auto-imagem, dente
outras qualidades. A utiliação de psudônimos é um costume muito apreciado em diversos meios,
e as pesquisas tem confirmado as vantagens que a prática apresenta e confirma\footnote{http://disqus.com/research/pseudonyms/}.

O autor destes textos se dá ao direito de não revelar seus psudônimos - embora muitos deles
sejam publicamente conhecidos - para conservar as consequências desta prática na
forma mais pura. Como comprovante desta produção, deixamos uma mensagem sobre a publicação
de textos em mídia impressa com autores internacionais,
confirmando a participação do autor desta dissertação, mas cujo
nome não consta na publicação.

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

de      fabi borges catadores@gmail.com por  riseup.net 
responder a     submidialogia@lists.riseup.net
para    submidialogia@lists.riseup.net
data    23 de agosto de 2011 12:26
assunto Re: [submidialogia] livro sub- publicação
lista de e-mails        <submidialogia.lists.riseup.net> Filtrar as mensagens dessa lista de e-mails
enviado por     lists.riseup.net
assinado por    riseup.net
cancelar inscrição      Cancelar a inscrição para essa lista de e-mails
        Importante principalmente porque você frequentemente lê mensagens com esse marcador.
ocultar detalhes 12:26 (6 minutos atrás)
entao, eu fui recebendo textos durante esse tempo,
alguns tao atrazados como dos sem satelites, mas muita gente mandou;

aqui os autores:

os internacionais nao sao muitos, o joni kempf (o que bebe ouro do hardware), o barbrook (futuros imaginarios),
Hamdy heda (da revolucao egipcia), o pedro soller (summerlab), maria llopis (pos porno),  talvez a bronac, nao entregou ainda.

dos brasileiros, renato fabri, ruiz, pasteur, morgana e caio, ju dornelles, mari marcassa, coletivo errorista, adriana velozo-drica, tiago pimentel, felipe fonseca, thiago novaes, lelex, felipe ribeiro(?), maira, verenilde,  bartolina silva, poro, vitoria amaro, fabib (eu),

entao, precisa publicar agora,
uma equipe para publicacao e,,, 

bjs
f

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

\end{itemize}

\subsection{Screencasts e outros materiais em video e em texto}

\begin{itemize}
    \item Python para áudio e música
	  Texto - palestras - videos

    \item Canal Macambira
No Macambira estão sendo produzidos materiais em screencasts sobre
diversas cenas de hackeamento.

    \begin{itemize}
	\item Live-Coding
	\item Raspagem de dados
    \end{itemize}
\end{itemize}

\subsection{Figusdevpack (FDP)}

Um ambiente de interação da comunidade de Python e Música 
para compartilhamento de códigos e excertos. Baseado principalmente
em documentação organizada sobre as práticas e as bibliotecas
existentes para python. Assim como scripts compartilhados que
fazem uso de objetos e módulos específicos. Este trabalho foi aceito na
maior conferência de áudio em linux, a Linux Audio Conference de 2008
(LAC2008) e está sendo reativado por mim em conjunto com Vilson Vieira
e outros desenvolvedores de áudio. Este projeto está em desenvolvimento
do site do Estúdio Livre [estudio livre] com repositório no sourceforge [source force].

As principais fontes sobre estre trabalho é a página de desenvolvimento da ideia
que está em constante mudança ~\cite{http://estudiolivre.org/tiki-index.php?page=fdp&highlight=fdp fdpel}
e, o artigo que foi aceito no LAC de 2008 ~\cite{http://www.estudiolivre.org/el-gallery_view.php?arquivoId=8221 fdplac2008}
e o repositório ~\cite{http://sourceforge.net/projects/fdpack/develop fdpsf}.
